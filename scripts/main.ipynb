{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch implementation of Deep Bilteral Learning for Real Time Image Enhancement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, unicode_literals\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os, sys, glob\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "#Torch Imports\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as data\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1920, 16])\n",
      "torch.Size([16, 1080])\n",
      "torch.Size([16, 1080])\n"
     ]
    }
   ],
   "source": [
    "size = (256, 256)\n",
    "batch_size = 100\n",
    "learning_rate = 0.01\n",
    "num_epochs = 5\n",
    "\n",
    "# size of image, right now we are fixing this\n",
    "x_size = 1920\n",
    "y_size = 1080\n",
    "\n",
    "# Constants for slicing layer\n",
    "sx = 256.0/x_size\n",
    "sy = 256.0/y_size\n",
    "d  = 8\n",
    "\n",
    "# tensorboard for pytorch stuff\n",
    "writer = SummaryWriter()\n",
    "sample_rate = 44100\n",
    "freqs = [262, 294, 330, 349, 392, 440, 440, 440, 440, 440, 440]\n",
    "\n",
    "# calculating Fx and keeping as constant from precomputation\n",
    "x = range(x_size)\n",
    "x = np.asarray(x, dtype=np.float64)\n",
    "one = np.ones(x_size)\n",
    "x = np.stack((x, one), axis=1)\n",
    "x[:, 0] = x[:, 0]*sx\n",
    "i  = range(16)\n",
    "i  = np.asarray(i, dtype=np.float64)*-1\n",
    "one= np.ones(16)\n",
    "i = np.stack((one, i), axis=1).swapaxes(0, 1)\n",
    "fx = torch.from_numpy(x)\n",
    "i  = torch.from_numpy(i)\n",
    "fxi= torch.matmul(fx, i)\n",
    "fxi= torch.abs(fxi)\n",
    "print(fxi.shape)\n",
    "\n",
    "# calculating Fy and keeping as constant from precomputation\n",
    "y = range(y_size)\n",
    "y = np.asarray(y, dtype=np.float64)\n",
    "one = np.ones(y_size)\n",
    "y = np.stack((y, one), axis=1)\n",
    "y[:, 0] = y[:, 0]*sy\n",
    "y = y.swapaxes(0, 1)\n",
    "i  = range(16)\n",
    "i  = np.asarray(i, dtype=np.float64)*-1\n",
    "one= np.ones(16)\n",
    "i = np.stack((one, i), axis=1)\n",
    "fy = torch.from_numpy(y)\n",
    "i  = torch.from_numpy(i)\n",
    "fyi= torch.matmul(i, fy)\n",
    "fyi= torch.abs(fyi)\n",
    "print(fyi.shape)\n",
    "print(fyi.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, root_dir, train, transform=None):\n",
    "        #Init Function\n",
    "        super(Dataset, self).__init__()\n",
    "        self.root_dir = root_dir\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.full_res = []\n",
    "        self.low_res = []\n",
    "        self.truth = []\n",
    "        \n",
    "        if (train):\n",
    "            dir = self.root_dir + '/train/'\n",
    "        else :\n",
    "            dir = self.root_dir + '/test/'\n",
    "        \n",
    "#         for img_path in glob.glob (dir + '*.jpg'):\n",
    "        for img_path in glob.glob (dir +'input/' '*.tif'):\n",
    "            img_name = img_path.split('/')[-1]\n",
    "            print (img_name)\n",
    "#             himage = Image.open (img_path)\n",
    "#             himage = himage.resize (img_w,img_h)           \n",
    "#             limage = himage.resize (size)           \n",
    "#             output = Image.open(dir+'output/'+img_name)\n",
    "#             output = output.resize(img_w,img_h)\n",
    "\n",
    "            himage = cv2.imread (img_path)\n",
    "            print(himage.shape)\n",
    "            himage = cv2.resize (himage,(x_size, y_size))\n",
    "            print(himage.shape)\n",
    "            limage = cv2.resize (himage,size)\n",
    "            output = cv2.imread (dir+'output/'+img_name)\n",
    "            output = cv2.resize (output,(x_size, y_size))\n",
    "\n",
    "            \n",
    "            self.full_res.append (himage)\n",
    "            self.low_res.append (limage)\n",
    "            self.truth.append (output)\n",
    "\n",
    "    def __len__(self):\n",
    "        #Length function ?\n",
    "        return len(self.full_res)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #Accessor Function\n",
    "        if self.transform is None:\n",
    "            return (self.full_res[idx],self.low_res[idx])\n",
    "        else:\n",
    "            limg_transformed = self.transform(self.low_res[idx])\n",
    "            himg_transformed =  self.transform(self.full_res[idx])\n",
    "            truth_transformed = self.transform(self.truth[idx])\n",
    "            return (himg_transformed, limg_transformed, truth_transformed)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a0001-jmac_DSC1459.tif\n",
      "(2000, 3008, 3)\n",
      "(1080, 1920, 3)\n",
      "a0004-jmac_MG_1384.tif\n",
      "(2912, 4368, 3)\n",
      "(1080, 1920, 3)\n",
      "a0001-jmac_DSC1459.tif\n",
      "(2000, 3008, 3)\n",
      "(1080, 1920, 3)\n"
     ]
    }
   ],
   "source": [
    "composed_transform = transforms.Compose([transforms.ToTensor()])\n",
    "# train_dataset = Dataset (root_dir = '../data', train = True, transform = composed_transform)\n",
    "train_dataset = Dataset (root_dir = '../dataset', train = True, transform = composed_transform)\n",
    "test_dataset = Dataset (root_dir = '../dataset', train = False, transform = composed_transform)\n",
    "# im, im2, im3 = train_dataset.__getitem__(0)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LocalFeatureNet (nn.Module):\n",
    "\n",
    "    def slicing_relu(self, F):\n",
    "        #Function similar to custom relu does the tau calculation\n",
    "        dimension = F.size()\n",
    "        F_temp = F.cuda()\n",
    "        F_temp = torch.ones(dimension).cuda().sub_(torch.abs(F_temp).float())\n",
    "        F_temp = F_temp.cpu().numpy()\n",
    "        F_temp[F_temp < 0] = 0\n",
    "        F_temp = torch.from_numpy(F_temp)\n",
    "        return F_temp\n",
    "    \n",
    "    def __init__(self):\n",
    "        super (LocalFeatureNet, self).__init__()\n",
    "        \n",
    "        self.relu  = nn.ReLU (inplace = True)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d (in_channels = 3,   out_channels = 8, kernel_size = 3, stride = 2, padding = 1)\n",
    "        self.conv2 = nn.Conv2d (in_channels = 8,  out_channels = 16,  kernel_size = 3, stride = 2, padding = 1)\n",
    "        self.conv3 = nn.Conv2d (in_channels = 16, out_channels = 32,  kernel_size= 3, stride = 2,padding = 1)\n",
    "        self.conv4 = nn.Conv2d (in_channels = 32, out_channels = 64,  kernel_size = 3, stride = 2, padding = 1)\n",
    "        \n",
    "        \n",
    "        self.localconv1 = nn.Conv2d (in_channels = 64, out_channels = 64,  kernel_size = 3, stride = 1, padding = 1)\n",
    "        self.localconv2 = nn.Conv2d (in_channels = 64, out_channels = 64,  kernel_size= 3, stride = 1, padding = 1)\n",
    "        \n",
    "        self.globalconv1 = nn.Conv2d (in_channels = 64, out_channels = 64,  kernel_size = 3, stride = 2,padding = 1)\n",
    "        self.globalconv2 = nn.Conv2d (in_channels = 64, out_channels = 64,  kernel_size = 3, stride = 2, padding = 1)\n",
    "        \n",
    "        self.globalfc1 = nn.Linear (1024, 256)\n",
    "        self.globalfc2 = nn.Linear (256, 128)\n",
    "        self.globalfc3 = nn.Linear (128, 64)\n",
    "        self.linear = nn.Conv2d(in_channels = 64, out_channels = 96,  kernel_size = 1, stride = 1)\n",
    "        \n",
    "        # Pixel Wise Network\n",
    "        self.pixelwise_bias = nn.Parameter (torch.rand(3, 1), requires_grad=True)\n",
    "        self.pixelwise_weight = nn.Parameter (torch.eye(3), requires_grad = True)\n",
    "        self.pixelwise_obias = nn.Parameter (torch.eye(1), requires_grad = True)\n",
    "        \n",
    "        self.relu_slopes = nn.Parameter(torch.rand(3,16), requires_grad = True)\n",
    "        self.relu_shifts = nn.Parameter(torch.rand(16,3), requires_grad = True)\n",
    "        \n",
    "        # Thing we do for upsampling\n",
    "        self.Fx = fxi\n",
    "        self.Fy = fyi\n",
    "        self.Fx = self.slicing_relu(self.Fx).cuda()\n",
    "        self.Fy = self.slicing_relu(self.Fy).cuda()\n",
    "        \n",
    "        \n",
    "    def custom_relu(self,channel,value):\n",
    "        print(\"value\",value.size())\n",
    "        size = (16,value.size()[0],value.size()[1],value.size()[2])\n",
    "        size_alt = (1L,value.size()[0],value.size()[1],value.size()[2])\n",
    "        print(\"size: \",size,\"size_alt\",size_alt)\n",
    "        value = value.expand(size)\n",
    "        \n",
    "#         print(self.relu_shifts[:,channel])\n",
    "        a = self.relu_shifts[:,channel].clone()\n",
    "        a = a.view(16,1,1,1)\n",
    "#         print(a.data,a)\n",
    "#         value = self.relu(value - a.repeat(size_alt))    \n",
    "# Upper line is not working for some damn reason so I hard coded it for now.\n",
    "#         print (\"here \", a.repeat(1,1,img_h,img_w).size())\n",
    "        value = self.relu(value - a.repeat(size[1],1, x_size, y_size))\n",
    "        print(\"st \", value.size())\n",
    "        print (\"st2 \", self.relu_slopes.size())\n",
    "        value = self.relu_slopes.matmul(value.view(x_size, y_size,16,-1))\n",
    "        print (\"stt \", value.size())\n",
    "        value = value.view(-1,3, x_size, y_size)\n",
    "        value = value[:,1,:,:]+value[:,2,:,:]+value[:,0,:,:]\n",
    "        print (\"stt2 \", value.size())\n",
    "        return value\n",
    "    \n",
    "    def upsample(self, grid, bilat):\n",
    "        \n",
    "        #### Making G\n",
    "        g = grid.cuda()\n",
    "        a = bilat.cuda()\n",
    "        print(type(bilat))\n",
    "        G = g.data\n",
    "        print(type(G))\n",
    "        G = d*G\n",
    "        one = torch.ones(G.shape).cuda()\n",
    "        print(type(one))\n",
    "        G = torch.stack([G, one], 3)\n",
    "        print(G.shape)\n",
    "        one = torch.ones(8)\n",
    "        K = torch.range(0, 7).cuda()* -1\n",
    "        one= torch.ones(8).cuda()\n",
    "        K = torch.stack((K, one), 1).permute(1, 0)\n",
    "        z = K[0]\n",
    "        K[0] = K[1]\n",
    "        K[1] = z\n",
    "#         G = torch.from_numpy(G)\n",
    "        F_gk = torch.matmul(G, K)\n",
    "        F_g = self.slicing_relu(F_gk)\n",
    "        print(\"**********************\")\n",
    "        print(F_gk.size())\n",
    "        print(F_g.size())\n",
    "        print(\"**********************\")\n",
    "        #########################################\n",
    "        \n",
    "        print(a.data.size())\n",
    "        A_temp = a.data.permute(0, 2, 3, 1, 4, 5)\n",
    "        F_prod = torch.matmul(self.Fx, A_temp)\n",
    "        F_prod = torch.matmul(F_prod, self.Fy)\n",
    "        print(\"**********************\")\n",
    "        print(F_prod.size())\n",
    "        print(\"**********************\")\n",
    "        F_prod_sum = F_prod.sum(3)\n",
    "        print(\"$$$$$$$$$$$$$$$\")\n",
    "        print(F_prod_sum.size())\n",
    "        print(\"**********************\")\n",
    "        a.data = F_prod_sum\n",
    "        return F_prod_sum\n",
    "        \n",
    "        \n",
    "        \n",
    "    def output(self, grid, inp):\n",
    "        out = torch.rand(inp.size()[0], y_size, x_size, 3).cuda()\n",
    "        out = 0 * out\n",
    "        grid = grid.view(-1, y_size, x_size, 12)\n",
    "        \n",
    "        for i in range(0,3):\n",
    "            out[:,:,:,i] = out[:,:,:,i].add_(grid[:,:,:,3+4*i])\n",
    "            for j in range(0,3):\n",
    "#                 print(grid[:,:,:,j+4*i].size())\n",
    "#                 print(inp[:,j,:,:].size())\n",
    "                a = torch.autograd.Variable(grid[:,:,:,j+4*i], requires_grad=False)\n",
    "#                 a = a.numpy()\n",
    "                b = inp[:,j,:,:]\n",
    "#                 b = b.numpy()\n",
    "                temp = a.data * b.data\n",
    "#                 print(temp.size())\n",
    "                out[:,:,:,i] = out[:,:,:,i] + temp\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def forward (self, h, l):\n",
    "        \n",
    "        x = self.relu (self.conv1 (l))\n",
    "        x = self.relu (self.conv2 (x))\n",
    "        x = self.relu (self.conv3 (x))\n",
    "        x = self.relu (self.conv4 (x))\n",
    "        y = self.localconv1 (x)\n",
    "        y = self.localconv2 (y)\n",
    "        z = self.globalconv1 (x)\n",
    "        z = self.globalconv2 (z)\n",
    "        z = self.globalfc1 (z.view(z.size()[0], -1))\n",
    "        z = self.globalfc2 (z)\n",
    "        z = self.globalfc3 (z)\n",
    "        fused = self.relu(z.view(-1,64,1,1)+y)\n",
    "        lin = self.linear(fused)\n",
    "        bilat = lin.view(-1, 8, 3, 4, 16, 16)\n",
    "#         print (bilat.size())\n",
    "\n",
    "# pixel wise network\n",
    "        for i in range(0,3):\n",
    "            a = self.pixelwise_weight[i,:].view(-1,3)\n",
    "            b = h.unsqueeze(0).view(3,-1)\n",
    "#             print(a.size(),b.size())\n",
    "            p = torch.mm(a,b)\n",
    "#             print(p.size())\n",
    "            p = p.view(h.size()[0],h.size()[2],h.size()[3]) + self.pixelwise_bias[i]\n",
    "#             print(p.size())\n",
    "#             p = torch.bmm(self.pixelwise_weight[i,:], h.unsqueeze(0).view(3,-1)).view(h.size()) + self.pixelwise_bias[i]\n",
    "#             print(i)\n",
    "            p += self.custom_relu(i,p)\n",
    "        p += self.pixelwise_obias\n",
    "#         print(p.size())\n",
    "        grid = p\n",
    "#         print(\"+++++++++++++++++++\")\n",
    "#         print(bilat.size())\n",
    "#         print(\"+++++++++++++++++++\")\n",
    "        print(grid.size())\n",
    "        bilat_new = self.upsample(grid, bilat)\n",
    "        #p is modified for tensorboard\n",
    "        p = p.view(p.size()[0],1,p.size()[1],p.size()[2])\n",
    "        writer.add_image('gridmap', p)\n",
    "        return self.output(bilat_new,h)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LocalFeatureNet() \n",
    "\n",
    "# Add code for using CUDA here if it is available\n",
    "use_gpu = False\n",
    "if(torch.cuda.is_available()):\n",
    "    use_gpu = True\n",
    "    model.cuda()\n",
    "\n",
    "# Loss function and optimizers\n",
    "criterion = torch.nn.MSELoss()# Define MSE loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)# Use Adam optimizer, use learning_rate hyper parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Code for training the model\n",
    "    # Make sure to output a matplotlib graph of training losses\n",
    "    loss_arr = []\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (himage, limage, truth) in enumerate(train_loader):  \n",
    "            \n",
    "            # Convert torch tensor to Variable\n",
    "            himage = Variable(himage)\n",
    "            limage = Variable(limage)\n",
    "            truth = Variable(truth)\n",
    "            if(use_gpu):\n",
    "                himage=himage.cuda()\n",
    "                limage=limage.cuda()\n",
    "                truth = truth.cuda()\n",
    "            \n",
    "            # Forward + Backward + Optimize\n",
    "            optimizer.zero_grad()  # zero the gradient buffer\n",
    "            outputs = model(himage, limage)\n",
    "            writer.add_image('himage', himage)\n",
    "            writer.add_image('limage', limage)\n",
    "            writer.add_image('truth',truth)\n",
    "            writer.add_image('output', outputs.view(-1,3, y_size, x_size))\n",
    "            loss = criterion(outputs, truth)\n",
    "            \n",
    "            model.pixelwise_obias.backward()\n",
    "            \n",
    "            model.pixelwise_bias[0,0].backward()\n",
    "            model.pixelwise_bias[1,0].backward()\n",
    "            model.pixelwise_bias[2,0].backward()\n",
    "            \n",
    "            model.pixelwise_weight[0,0].backward()\n",
    "            model.pixelwise_weight[0,1].backward()\n",
    "            model.pixelwise_weight[0,2].backward()\n",
    "            model.pixelwise_weight[1,0].backward()\n",
    "            model.pixelwise_weight[1,1].backward()\n",
    "            model.pixelwise_weight[1,2].backward()\n",
    "            model.pixelwise_weight[2,0].backward()\n",
    "            model.pixelwise_weight[2,1].backward()\n",
    "            model.pixelwise_weight[2,2].backward()\n",
    "            \n",
    "            for i in range(3):\n",
    "                for j in range(16):\n",
    "                    model.relu_slopes[i,j].backward()\n",
    "                    model.relu_shifts[j,i].backward()\n",
    "                    \n",
    "#             loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_arr.append(loss.data[0])\n",
    "            writer.add_scalar('loss',loss.data[0])\n",
    "            if (i+1) % batch_size == 0:       \n",
    "                print ('Epoch [%d/%d], Step [%d/%d], Loss: %.4f' \n",
    "                       %(epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.data[0]))\n",
    "    \n",
    "    plt.plot( np.array(range(1,len(loss_arr)+1)), np.array(loss_arr))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value torch.Size([2, 1080, 1920])\n",
      "size:  (16, 2L, 1080L, 1920L) size_alt (1L, 2L, 1080L, 1920L)\n",
      "st  torch.Size([16, 2, 1080, 1920])\n",
      "st2  torch.Size([3, 16])\n",
      "stt  torch.Size([1920, 1080, 3, 2])\n",
      "stt2  torch.Size([2, 1920, 1080])\n",
      "value torch.Size([2, 1080, 1920])\n",
      "size:  (16, 2L, 1080L, 1920L) size_alt (1L, 2L, 1080L, 1920L)\n",
      "st  torch.Size([16, 2, 1080, 1920])\n",
      "st2  torch.Size([3, 16])\n",
      "stt  torch.Size([1920, 1080, 3, 2])\n",
      "stt2  torch.Size([2, 1920, 1080])\n",
      "value torch.Size([2, 1080, 1920])\n",
      "size:  (16, 2L, 1080L, 1920L) size_alt (1L, 2L, 1080L, 1920L)\n",
      "st  torch.Size([16, 2, 1080, 1920])\n",
      "st2  torch.Size([3, 16])\n",
      "stt  torch.Size([1920, 1080, 3, 2])\n",
      "stt2  torch.Size([2, 1920, 1080])\n",
      "torch.Size([2, 1080, 1920])\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "<class 'torch.cuda.FloatTensor'>\n",
      "<class 'torch.cuda.FloatTensor'>\n",
      "torch.Size([2, 1080, 1920, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eeshangd/vision_env/lib/python2.7/site-packages/ipykernel_launcher.py:88: UserWarning: torch.range is deprecated in favor of torch.arange and will be removed in 0.3. Note that arange generates values in [start; end), not [start; end].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********************\n",
      "torch.Size([2, 1080, 1920, 8])\n",
      "torch.Size([2, 1080, 1920, 8])\n",
      "**********************\n",
      "torch.Size([2, 8, 3, 4, 16, 16])\n",
      "**********************\n",
      "torch.Size([2, 3, 4, 8, 1920, 1080])\n",
      "**********************\n",
      "$$$$$$$$$$$$$$$\n",
      "torch.Size([2, 3, 4, 1920, 1080])\n",
      "**********************\n",
      "value torch.Size([2, 1080, 1920])\n",
      "size:  (16, 2L, 1080L, 1920L) size_alt (1L, 2L, 1080L, 1920L)\n",
      "st  torch.Size([16, 2, 1080, 1920])\n",
      "st2  torch.Size([3, 16])\n",
      "stt  torch.Size([1920, 1080, 3, 2])\n",
      "stt2  torch.Size([2, 1920, 1080])\n",
      "value torch.Size([2, 1080, 1920])\n",
      "size:  (16, 2L, 1080L, 1920L) size_alt (1L, 2L, 1080L, 1920L)\n",
      "st  torch.Size([16, 2, 1080, 1920])\n",
      "st2  torch.Size([3, 16])\n",
      "stt  torch.Size([1920, 1080, 3, 2])\n",
      "stt2  torch.Size([2, 1920, 1080])\n",
      "value torch.Size([2, 1080, 1920])\n",
      "size:  (16, 2L, 1080L, 1920L) size_alt (1L, 2L, 1080L, 1920L)\n",
      "st  torch.Size([16, 2, 1080, 1920])\n",
      "st2  torch.Size([3, 16])\n",
      "stt  torch.Size([1920, 1080, 3, 2])\n",
      "stt2  torch.Size([2, 1920, 1080])\n",
      "torch.Size([2, 1080, 1920])\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "<class 'torch.cuda.FloatTensor'>\n",
      "<class 'torch.cuda.FloatTensor'>\n",
      "torch.Size([2, 1080, 1920, 2])\n",
      "**********************\n",
      "torch.Size([2, 1080, 1920, 8])\n",
      "torch.Size([2, 1080, 1920, 8])\n",
      "**********************\n",
      "torch.Size([2, 8, 3, 4, 16, 16])\n",
      "**********************\n",
      "torch.Size([2, 3, 4, 8, 1920, 1080])\n",
      "**********************\n",
      "$$$$$$$$$$$$$$$\n",
      "torch.Size([2, 3, 4, 1920, 1080])\n",
      "**********************\n",
      "value torch.Size([2, 1080, 1920])\n",
      "size:  (16, 2L, 1080L, 1920L) size_alt (1L, 2L, 1080L, 1920L)\n",
      "st  torch.Size([16, 2, 1080, 1920])\n",
      "st2  torch.Size([3, 16])\n",
      "stt  torch.Size([1920, 1080, 3, 2])\n",
      "stt2  torch.Size([2, 1920, 1080])\n",
      "value torch.Size([2, 1080, 1920])\n",
      "size:  (16, 2L, 1080L, 1920L) size_alt (1L, 2L, 1080L, 1920L)\n",
      "st  torch.Size([16, 2, 1080, 1920])\n",
      "st2  torch.Size([3, 16])\n",
      "stt  torch.Size([1920, 1080, 3, 2])\n",
      "stt2  torch.Size([2, 1920, 1080])\n",
      "value torch.Size([2, 1080, 1920])\n",
      "size:  (16, 2L, 1080L, 1920L) size_alt (1L, 2L, 1080L, 1920L)\n",
      "st  torch.Size([16, 2, 1080, 1920])\n",
      "st2  torch.Size([3, 16])\n",
      "stt  torch.Size([1920, 1080, 3, 2])\n",
      "stt2  torch.Size([2, 1920, 1080])\n",
      "torch.Size([2, 1080, 1920])\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "<class 'torch.cuda.FloatTensor'>\n",
      "<class 'torch.cuda.FloatTensor'>\n",
      "torch.Size([2, 1080, 1920, 2])\n",
      "**********************\n",
      "torch.Size([2, 1080, 1920, 8])\n",
      "torch.Size([2, 1080, 1920, 8])\n",
      "**********************\n",
      "torch.Size([2, 8, 3, 4, 16, 16])\n",
      "**********************\n",
      "torch.Size([2, 3, 4, 8, 1920, 1080])\n",
      "**********************\n",
      "$$$$$$$$$$$$$$$\n",
      "torch.Size([2, 3, 4, 1920, 1080])\n",
      "**********************\n",
      "value torch.Size([2, 1080, 1920])\n",
      "size:  (16, 2L, 1080L, 1920L) size_alt (1L, 2L, 1080L, 1920L)\n",
      "st  torch.Size([16, 2, 1080, 1920])\n",
      "st2  torch.Size([3, 16])\n",
      "stt  torch.Size([1920, 1080, 3, 2])\n",
      "stt2  torch.Size([2, 1920, 1080])\n",
      "value torch.Size([2, 1080, 1920])\n",
      "size:  (16, 2L, 1080L, 1920L) size_alt (1L, 2L, 1080L, 1920L)\n",
      "st  torch.Size([16, 2, 1080, 1920])\n",
      "st2  torch.Size([3, 16])\n",
      "stt  torch.Size([1920, 1080, 3, 2])\n",
      "stt2  torch.Size([2, 1920, 1080])\n",
      "value torch.Size([2, 1080, 1920])\n",
      "size:  (16, 2L, 1080L, 1920L) size_alt (1L, 2L, 1080L, 1920L)\n",
      "st  torch.Size([16, 2, 1080, 1920])\n",
      "st2  torch.Size([3, 16])\n",
      "stt  torch.Size([1920, 1080, 3, 2])\n",
      "stt2  torch.Size([2, 1920, 1080])\n",
      "torch.Size([2, 1080, 1920])\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "<class 'torch.cuda.FloatTensor'>\n",
      "<class 'torch.cuda.FloatTensor'>\n",
      "torch.Size([2, 1080, 1920, 2])\n",
      "**********************\n",
      "torch.Size([2, 1080, 1920, 8])\n",
      "torch.Size([2, 1080, 1920, 8])\n",
      "**********************\n",
      "torch.Size([2, 8, 3, 4, 16, 16])\n",
      "**********************\n",
      "torch.Size([2, 3, 4, 8, 1920, 1080])\n",
      "**********************\n",
      "$$$$$$$$$$$$$$$\n",
      "torch.Size([2, 3, 4, 1920, 1080])\n",
      "**********************\n",
      "value torch.Size([2, 1080, 1920])\n",
      "size:  (16, 2L, 1080L, 1920L) size_alt (1L, 2L, 1080L, 1920L)\n",
      "st  torch.Size([16, 2, 1080, 1920])\n",
      "st2  torch.Size([3, 16])\n",
      "stt  torch.Size([1920, 1080, 3, 2])\n",
      "stt2  torch.Size([2, 1920, 1080])\n",
      "value torch.Size([2, 1080, 1920])\n",
      "size:  (16, 2L, 1080L, 1920L) size_alt (1L, 2L, 1080L, 1920L)\n",
      "st  torch.Size([16, 2, 1080, 1920])\n",
      "st2  torch.Size([3, 16])\n",
      "stt  torch.Size([1920, 1080, 3, 2])\n",
      "stt2  torch.Size([2, 1920, 1080])\n",
      "value torch.Size([2, 1080, 1920])\n",
      "size:  (16, 2L, 1080L, 1920L) size_alt (1L, 2L, 1080L, 1920L)\n",
      "st  torch.Size([16, 2, 1080, 1920])\n",
      "st2  torch.Size([3, 16])\n",
      "stt  torch.Size([1920, 1080, 3, 2])\n",
      "stt2  torch.Size([2, 1920, 1080])\n",
      "torch.Size([2, 1080, 1920])\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "<class 'torch.cuda.FloatTensor'>\n",
      "<class 'torch.cuda.FloatTensor'>\n",
      "torch.Size([2, 1080, 1920, 2])\n",
      "**********************\n",
      "torch.Size([2, 1080, 1920, 8])\n",
      "torch.Size([2, 1080, 1920, 8])\n",
      "**********************\n",
      "torch.Size([2, 8, 3, 4, 16, 16])\n",
      "**********************\n",
      "torch.Size([2, 3, 4, 8, 1920, 1080])\n",
      "**********************\n",
      "$$$$$$$$$$$$$$$\n",
      "torch.Size([2, 3, 4, 1920, 1080])\n",
      "**********************\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEDCAYAAADZUdTgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzt3XlwlIed5vHvTwfiPiVAqIUFxjY+\nuFvYiRPHxk5iG0ywAeFNZibZTco11062UpvMZrdqjkxt7WS2asbJbs1mUp4juzOZSDK+zzhjHDtO\nbKvFjfGBAdMSAon7Rkj67R/9ylaIhFpXv939Pp+qLrpbb3f/eJEeXr3v20+buyMiItFQEPYAIiKS\nOQp9EZEIUeiLiESIQl9EJEIU+iIiEaLQFxGJkKwNfTP7BzNrNbOdw/R8f2Vmu8xst5l938wszcfd\nZmabzazDzNZdYbn/bmZJMzvTx9fXmpmbWTy4/SUz29rj0mVmi4OvbTCz7cG83x3oLANhZuuD1+nq\nnk1E8lfWhj7wT8Ddw/FEZvZJ4FZgIXATUA185rJlqszslV4efgD4CvDjfl7maWB5H68/Afg68Gb3\nfe7+L+6+2N0XA78N7HP3rWY2DfifwJ3ufiMw08zuHOAsA7ETeAB4dRifU0SyVNaGvru/ChzreZ+Z\nXW1mL5hZo5m9Zmbz0306YDQwCigBioHDac6x3923A139LPeGu7f08eW/AL4LXOjj6/8O+ElwfS7w\nvru3Bbd/BqztbxYz+6aZNQS/Ifz5Ff9Svz73bnd/N93lRSS3ZW3o9+GHwH9092XAfwb+Np0Hufuv\ngE1AS3B50d13j9iUPZjZUqDS3Z+9wmIbgH8Nru8Brgt+8ygC1gCV/bzG54BrSP2msRhYZma3DXl4\nEck7RWEPkC4zGw98EqjvsTu+JPjaA8B3enlYs7t/3szmAdcDseD+l8zs0+7+mpk9Dswh9VvAbDPb\nGizzPXf/xyHOXAD8NaldMn0tczNwzt13Arj7cTP7PaCW1Bb9L4Gr+3mpzwWXLcHt8aT+E3jVzH4G\nzOzlMf/N3Z9M/28jIvkgZ0Kf1G8lJ4J94L/G3R8DHrvCY+8H3nD3MwBm9jzwCeA1d78/uK8K+Cd3\nv30YZ55A6hjCK8F/VDOBp8xstbsngmUe5OOtfADc/WlSxwgws4eAzn5ex4D/4e5/d/kX3P2uIf0N\nRCSv5MzuHXc/Bewzs/UAlrIozYcfAD5jZkVmVkzqIO6I795x95PuXuruVe5eBbwBfBT4wW8CNXy8\nP5/g/unBn1OA3wce6eelXgT+Q/DbEGZW0f0cIiI9ZW3om9m/Ar8itX+7ycy+CnwJ+KqZbQN2AV9I\n8+keBT4AdgDbgG3B1nQ6c1SbWROwHvg7M9vV42tbe1z/q2C5scG8f5bG098GJN1972X3f8/M3gZe\nB/7S3d+70izu/lNSZ/T8ysx2BH/fCWn+/e4PnvMTwLNm9mI6jxOR3GSqVhYRiY6s3dIXEZHhl3UH\ncktLS72qqirsMUREckpjY+MRdy/rb7msC/2qqioSiUT/C4qIyEfM7MN0ltPuHRGRCFHoi4hEiEJf\nRCRCFPoiIhGi0BcRiRCFvohIhCj0RUQiJOvO0x+sE+fa+cfX93PvgnKum5lW7YyIjKBX32sjsf9Y\n/wvKR2ZOGsMXb549oq+RN6EP8H9+/gEnz1/iz1bfGPYoIpF24VInf/jjzZy60EF6n0YtAIsrJyv0\n0zV57Cg+f+NMHt/SzH+5Zz6jiwvDHkkksl7cdYhTFzr4l6/dzK3zSsMeR3rIq336NfEYJ89f4qW3\n0/r4WxEZIXWJJLEpY/jE3GlhjyKXSTv0zazQzLaY2TPB7T80sz1m5mbW53/lZtZpZluDy1PDMXRf\nbr26lIrJY6hLJEfyZUTkCpLHzvH6nqOsX1ZJQYH27WSbgWzpf51f/7Sp14G7gP5Kfs67++Lgsnqg\nAw5EQYGxblmMX+w5QtPxcyP5UiLSh/rGJsxgXTzW/8KScWmFvpnFgJX0+Ng+d9/i7vtHaK5BWx98\no21sbA55EpHo6exyHk0k+dS81G/dkn3S3dJ/GPgW0DWI1xhtZgkze8PM1vS2gJk9FCyTaGtrG8RL\nfCw2ZSy3Xl1KfWOSri59KphIJr2+5wgHT15gQ3Vl2KNIH/oNfTNbBbS6e+MgX+Mqd48DXwQeNrOr\nL1/A3X/o7nF3j5eV9fsZAP2qqa6k6fh5frX36JCfS0TSV5tIMnlsMZ+9YUbYo0gf0tnSvxVYbWb7\ngZ8AK8zsn9N9AXdvDv7cC7wCLBn4mAPzuRtmMGlMMbUNOqArkinHz7bz0q7DrFlcQUmRTpnOVv2G\nvrt/291j7l4FPAi87O6/lc6Tm9kUMysJrpeS+g/k7SHMm5bRxYWsWTyLF3Yd4uS5SyP9ciICPLG1\nmfbOLmri2rWTzQZ9nr6Z/ZGZNQExYLuZPRLcH+++DlwPJMxsG7AJ+Et3H/HQB1gfr6S9o4snt+mA\nrshIc3dqG5IsqJjEDbMmhj2OXMGA3pHr7q+Q2kWDu38f+H4vyySArwXXfwksGOqQg3FTxSRunDWR\n2oYkv/OJqjBGEImMnc2neOfQaf5izU1hjyL9yKt35F6uJl7JroOn2Nl8MuxRRPJaXSJJSVEBqxfN\nCnsU6Udeh/6axRWMKiqgXu/QFRkxFy518sTWZu65aSaTxhSHPY70I69Df9LYYu6+cSZPbD3IhUud\nYY8jkpde3HWI0xc6dAA3R+R16ENqF8/J85f4qUrYREZEbUOSyqljuEXlajkh70P/k1dPo2LyGO3i\nERkByWPn+OUHKlfLJXkf+gUFxvq4SthERkJ9IpkqV1umcrVckfehDx9/Qz7a2BTyJCL5o7PLebSx\niU9fU8YslavljEiEfmzKWD41r5T6RJNK2ESGyS+6y9V0ADenRCL0IXVAt/nEeX75gUrYRIZDXUOS\nKWOLueuG6WGPIgMQmdD/bHcJmw7oigzZsbPt/PTtQ6xZonK1XBOZ0B9dXMj9Syp4cdchTpxrD3sc\nkZz2xJZmLnW6evNzUGRCH1KfqtXe0cWTWw+GPYpIznJ36hJJFsYmMX+mytVyTaRC/8ZZk7ipYqJ6\n9kWGYEfzSd45dFrvwM1RkQp9SB3QfbtFJWwig9VdrnafytVyUuRC/wuLUiVsdTqgKzJgFy518uTW\ng9y7oFzlajkqcqE/aWwx99w0kye2NKuETWSAXtiZKldbH9c7cHNV5EIfUrt4Tl3o4MVdh8IeRSSn\n1DYkmT11LLfMUblaropk6H9i7jRiU8ZQn1Atg0i6Dhw9x6/2HmX9spjK1XJYJEO/oMBYv6ySX+w5\nQvKYSthE0lHfmKTAYJ127eS0SIY+pL5xzVTCJpKO7nK1264to3ySytVyWWRDv2LyGD41r5RHG5vo\nVAmbyBW99n4bLScv6Nz8PBDZ0AfYUJ0qYXt9z5GwRxHJanWJJFPHjeKu62eEPYoMUaRD/7M3zGDy\n2GKdsy9yBcfOtvPS24dZszj1HhfJbZH+FywpKmTN4gp+uuswx8+qhE2kN4+rXC2vRDr0IXXOfntn\nF09ubQ57FJGs4+7UJ5Isik3iupkTwh5HhkHkQ/+GWRNZUDGJ2kQT7jqgK9LT9qagXE1b+Xkj8qEP\nUFNdye6WU+w6eCrsUUSySl0iyehilavlE4U+sHrRLEqKClS5LNLD+fZOntp6kHtvKmfiaJWr5QuF\nPjBpTKqE7cmtKmET6fbCrhZOX+xgvc7NzysK/YBK2ER+XW1DkqumjeWWuVPDHkWGkUI/cMvcaVRO\nHaNz9kWAD4+e5Y29x1i/LIaZytXyiUI/0F3C9vqeoyphk8irTzSlytWWaddOvlHo97BuWaqErV5b\n+xJh3eVqn7m2jJmTRoc9jgwzhX4PsyaP4dPXlKmETSLt1ffbOHRK5Wr5SqF/mQ3xSg6evMAvVMIm\nEVXXkCpXu1Plankp7dA3s0Iz22JmzwS3/9DM9piZm1npFR73ZTN7P7h8eTiGHkl33TCdKSphk4g6\neuYiP9t9mPuXqFwtXw3kX/XrwO4et18H7gI+7OsBZjYV+FPgZmA58KdmNmUQc2ZMSVEha5ZU8JJK\n2CSCusvVtGsnf6UV+mYWA1YCj3Tf5+5b3H1/Pw/9PPCSux9z9+PAS8Ddg5w1Y7pL2J5QCZtEiLtT\nl0iyqHKyytXyWLpb+g8D3wK6Bvj8FUDP/SRNwX2/xsweMrOEmSXa2toG+BLD7/ryiSyMTaK2IakS\nNomMbU0nee/wGTZoKz+v9Rv6ZrYKaHX3xpEawt1/6O5xd4+XlZWN1MsMSE28kncOnWZns0rYJBq6\ny9VWLSoPexQZQels6d8KrDaz/cBPgBVm9s9pPn8z0HOzIRbcl/Xu6y5hSxwIexSREXe+vZOntx7k\n3gUqV8t3/Ya+u3/b3WPuXgU8CLzs7r+V5vO/CHzOzKYEB3A/F9yX9SaNKebeBeU8ufWgStgk7z2/\nM1WupgO4+W/Q52SZ2R+ZWROprfftZvZIcH+8+7q7HwP+AmgILt8J7ssJ6+MxTl/o4IWdKmGT/Fbb\nkKRq2lhunqNytXw3oNB391fcfVVw/fvBbwBF7j7L3b8W3J/ovh7c/gd3nxdc/nF4xx9Zt8yZxuyp\nY3XOvuS1/UfO8ua+Y6yPV6pcLQL07osrSJWwxfjlB0c5cFQlbJKf6huTFBisXRoLexTJAIV+P9bF\ngxK2Rm3tS/7p6Ozi0cYmbr9uusrVIkKh34/ySWO4TSVskqdee/8Ih09dpCaurfyoUOinYUN1JS0n\nL/Da++G/cUxkONU2JJk2bhQr5qtcLSoU+mm48/pUCVt9oinsUUSGjcrVokn/0mkoKSrk/iUxfvr2\nIY6phE3yxONbmunocmqqdW5+lCj001RTHeNSp/PElpx4Q7HIFbk7tQ1JFldO5toZKleLEoV+mubP\nnMii2CTqEiphk9y3NXmC91vPsEFb+ZGj0B+AmupUCduO5pNhjyIyJHWJJsYUF7JqocrVokahPwD3\nLZrF6OICaht0zr7krnPtHTy9LVWuNkHlapGj0B+AiaOLufemcp7aepDz7Sphk9z03I5DnLnYoXPz\nI0qhP0Dr45WcvtjBC7tawh5FZFDqEknmlI5jucrVIkmhP0C3zJ3KVdPGaheP5KR9R87y1r5jrI/H\nVK4WUQr9ATJLlbC9sfcYHx49G/Y4IgNSn1C5WtQp9Adh7bIYBYbeoSs5pbtc7Y7rpjNjosrVokqh\nPwjlk8Zw27UqYZPc8ur7bbSevsh6fTpWpCn0B2lDvJJDpy7wqkrYJEfUNiQpHT+KO6+fHvYoEiKF\n/iDdef0Mpo4bRb0+VUtywJEzF/m33a3cv6SC4kL92EeZ/vUHaVRRAfcvqeCltw9z9MzFsMcRuaLH\nNwflatq1E3kK/SGoiVemSti2Hgx7FJE+uTt1iSRLZk/mGpWrRZ5CfwiumzmBRZWTqWtQCZtkry3d\n5WrayhcU+kO2IV7Ju4dPs71JJWySneoTScYUF7JS5WqCQn/IVi0qT5Ww6YCuZKFUuVoLKxeqXE1S\nFPpDNHF0MfcuKOdplbBJFnp2ewtnLnaoN18+otAfBjVBCdvzO1XCJtmlPtHE3NJxxK+aEvYokiUU\n+sPg5jlTqVIJm2SZvW1neGv/MdbHK1WuJh9R6A8DM2N9vJI39x1j/xGVsEl2qG9sorDAWLu0IuxR\nJIso9IfJ2qVBCVujtvYlfB2dXWxsbOKO68qYrnI16UGhP0xmThrNZ1TCJlni5++pXE16p9AfRhuq\nKzl86iKvvqcSNglXd7naivkqV5Nfp9AfRivmz2DauFHU6Zx9CVHb6Yu8/E4rDyyNqVxNfoO+I4ZR\ndwnbz3arhE3C8/iWpqBcTZ+OJb9JoT/MaqpTJWyPb2kOexSJoFS5WhNLZ09m3nSVq8lvUugPs2tn\nTGBx5WTqEiphk8zbfOAEe1rP6B240ieF/gjYUF3Je4fPsE0lbJJhdQ1Jxo4qZOXCWWGPIlkq7dA3\ns0Iz22JmzwS355jZm2a2x8xqzWxUL4+pMrPzZrY1uPxgOIfPVqsWljOmuFDv0JWMOnuxg2e2H2Tl\ngnLGlxSFPY5kqYFs6X8d2N3j9neBv3H3ecBx4Kt9PO4Dd18cXH53kHPmlAndJWzbDnKuvSPscSQi\nnt3Rwtn2Tu3akStKK/TNLAasBB4JbhuwAng0WORHwJqRGDBX1cRjnLnYwfM7DoU9ikREfSLJ3LJx\nLFO5mlxBulv6DwPfArqC29OAE+7evRnbBPRV8DEn2C30czP7dG8LmNlDZpYws0RbW368sWl5dwmb\nztmXDPig7QwN+49To3I16Ue/oW9mq4BWd28cxPO3ALPdfQnwDeDHZjbx8oXc/YfuHnf3eFlZ2SBe\nJvt0l7C9te8Y+1TCJiOsPpEqV3tA5WrSj3S29G8FVpvZfuAnpHbrfA+YbGbdR4tiwG+cmO7uF939\naHC9EfgAuHYY5s4J65YFJWza2pcR1NHZxcbNTdxx3XSmT1C5mlxZv6Hv7t9295i7VwEPAi+7+5eA\nTcC6YLEvA09e/lgzKzOzwuD6XOAaYO8wzZ71Zkwcze3XTWfj5iY6Orv6f4DIILzybhttpy/qHbiS\nlqGcp//HwDfMbA+pffx/D2Bmq83sO8EytwHbzWwrqYO+v+vux4YycK6piQclbO/nx7EKyT61iSSl\n40u4Q+VqkoYBnczr7q8ArwTX9wLLe1nmKeCp4PpGYONQh8xld14/ndLxo6hraGLF/BlhjyN5pvX0\nBV5+p5WvfWqOytUkLfouGWHFhR+XsB1RCZsMs8c3N9PZ5erNl7Qp9DOgJl5JR5fzhErYZBilytWS\nLLtqCvOmjw97HMkRCv0MuGbGBJbMnkxtg0rYZPhsPnCcD9rOskFb+TIACv0M2RCv5P3WM2xJngh7\nFMkTtR+Vq5WHPYrkEIV+hqwMSth0zr4Mh1S5WgurFpYzTuVqMgAK/QyZMLqYlQvLeXpbi0rYZMie\n3d7COZWrySAo9DOoJl7JmYsdPKcSNhmiuqBcbelslavJwCj0M6i6agpzSsdRp559GYI9rWdIfHic\nDSpXk0FQ6GdQqoQtxlv7j7G37UzY40iOqm9MUlhg3K9yNRkEhX6GrVsao7DAqG9sCnsUyUGXOrvY\n2NjMivkqV5PBUehn2PSJo7n92jI2NqqETQbulXfbOHLmIjU6N18GSaEfgprqSlpPX+Tn76mETQam\ntiFJ2YQS7rguPz53QjJPoR+CFfODEjadsy8D0Hr6ApvebeWBpRUUqVxNBknfOSEoLizggaUx/m13\nK22nVcIm6XksKFfTrh0ZCoV+SGriMZWwSdrcnbqGJNVVU7i6TOVqMngK/ZDMmz6BpbMnU5tQCZv0\nr/HD4+w9clYVyjJkCv0QbaiuZE/rGTYfUAmbXFltQ5JxowpZuUDlajI0Cv0QrVw4i7GjVMImV3bm\nYgfP7mhh1cJZKleTIVPoh2h8SRErF5Tz9LaDnL2oEjbp3bPbD3KuvZMalavJMFDoh6ymupKz7Z08\nt6Ml7FEkS9Ulmri6bBxLZ08OexTJAwr9kMWvmsLc0nE6Z196taf1NI0fHmdDtcrVZHgo9EOWKmGr\npGH/cZWwyW+oTzRRVGDcvyQW9iiSJxT6WWDt0goKC4y6hErY5GOXOrvYuLmJFfOnUzahJOxxJE8o\n9LPA9ImjueO6MjZuVgmbfGzTO60cOdOud+DKsFLoZ4maeCVtpy/yyrsqYZOUukSqXO12lavJMFLo\nZ4k75k+ndHyJDugKAK2nLrDp3TbWLo2pXE2Glb6bskRxYQFrl1bw8jsqYRPY+FG5mg7gyvBS6GeR\n9fFKOrqcxzbrgG6UuTv1iSTLq6YyV+VqMswU+llk3vTxLLtqCnUqYYu0xEflatrKl+Gn0M8yG+KV\nfNB2ls0Hjoc9ioTko3K1hSpXk+Gn0M8y9y4sZ+yoQuoatIsnis5c7ODZ7S3ct2gWY0epXE2Gn0I/\ny4wvKWLVwnKe2a4Stih6ZttBzl9SuZqMHIV+FqqJp0rYnlUJW+TUJZLMmz6eJZUqV5ORodDPQsuu\nmsLcsnHUNeic/SjZ03qazQdOsCGucjUZOQr9LGRm1MQrSXx4nA9UwhYZdd3laksrwh5F8ljaoW9m\nhWa2xcyeCW7PMbM3zWyPmdWa2ag+HvftYJl3zezzwzV4vnvgoxI2be1HwaXOLh7b3MSd16femS0y\nUgaypf91YHeP298F/sbd5wHHga9e/gAzuwF4ELgRuBv4WzMrHPy40TF9wmhWzJ/OxsZmLqmELe+9\nrHI1yZC0Qt/MYsBK4JHgtgErgEeDRX4ErOnloV8AfuLuF919H7AHWD7UoaOiJl7JkTMqYYuCuoYk\n0yeU8JlrVa4mIyvdLf2HgW8B3Zuc04AT7t59TmET0NuOyAqg5/6JXpczs4fMLGFmibY2BVy3O64r\no2yCStjy3eFTF9j0bivrlqlcTUZev99hZrYKaHX3xpEawt1/6O5xd4+XlWlLp1tRYQEPBCVsracv\nhD2OjJCNm5vo8lT3kshIS2ez4lZgtZntB35CarfO94DJZtb9lsEY0NzLY5uBnt/JfS0nfaiJV9LZ\n5Ty2WastH6XK1ZpYPmcqc0rHhT2OREC/oe/u33b3mLtXkToo+7K7fwnYBKwLFvsy8GQvD38KeNDM\nSsxsDnAN8NawTB4RV5eNJ64StrzVsP84+46c1QFcyZih7ED8Y+AbZraH1D7+vwcws9Vm9h0Ad98F\n1AFvAy8Af+DunUMbOXpqqivZ23aWxg9VwpZvahuSjC8p4t4FM8MeRSJiQKHv7q+4+6rg+l53X+7u\n89x9vbtfDO5/yt3/pMdj/ru7X+3u17n788M7fjSsXFDOuFGFOqCbZ05fuMRzO1q4b1G5ytUkY3Sq\nQA4YV1LEqoWzeGZ7C2dUwpY3ntnekipX064dySCFfo6oqY5xrr2T57arhC1f1CWSXDN9PItVriYZ\npNDPEUtnT+HqsnHUahdPXnj/8Gm2HDjBhmqVq0lmKfRzRHcJW+OHx9nTqhK2XFeXSFJUYKxZonI1\nySyFfg55YGmMogKjXlv7Oa29o4vHNjdz1/UzVK4mGafQzyFlE0pSJWybVcKWy15+p5WjZ9upqdYH\nn0vmKfRzTHcJ26Z3WsMeRQapLpFkxsQSbrtGlSOSeQr9HHO7Sthy2qGTF3hF5WoSIn3X5ZiiwgLW\nLo2x6d02Wk+phC3XfFSutkzn5ks4FPo5qCYeo7PL2agStpySKldLcvOcqVSpXE1CotDPQXPLxlNd\nNYV6lbDllLf2HWP/0XN6B66ESqGfo2rilew9cpaESthyRm2iu1ytPOxRJMIU+jnq3u4StgYd0M0F\nH5erzWLMKH1MtIRHoZ+jxpUUcd+iWTy7QyVsueDpbS1cuNTFhmrt2pFwKfRz2Pp4JefaO3l2+8Gw\nR5F+1CWSXDtjPItik8IeRSJOoZ/Dls6ezLzp46nVLp6s9t7h02xNnqAmrnI1CZ9CP4elSthibD5w\ngj2tp8MeR/pQ15CkuNC4X+VqkgUU+jmuu4StLtEU9ijSi/aOLh7bkipXm6ZyNckCCv0cVzq+hDuv\nn85jm5tUwpaF/m33YY6dbadGB3AlSyj080CqhK2dl1XClnXqEklmThytcjXJGgr9PPCZa8uYPqFE\n5+xnmUMnL/Dz99pYtyxGYYEO4Ep2UOjngaLCAtYui7Hp3VYOq4Qta3xUrhZXb75kD4V+nqiJV9Ll\nqaCR8HV1OXWJJLfMncpV01SuJtlDoZ8n5pSOY3nVVOoTTSphywJv7T/GhypXkyyk0M8jNdWV7Dty\nlob9KmELW11DkgklRdxzk8rVJLso9PPIvQtmMr6kSJ+qFbJTFy7x3M4W7luscjXJPgr9PDJ2VBH3\nLSrn2e0tnL5wKexxIuvpbQdT5WratSNZSKGfZ9bHKzl/qZNnt7eEPUpk1SWauG7GBBaqXE2ykEI/\nzyypnMw108dTq108oXj30Gm2JU9QU61yNclOCv08Y2ZsqK5ky4ETvH9YJWyZVpdQuZpkN4V+Hlqz\npCIoYdPWfia1d3Tx+JZmPnvDDKaOGxX2OCK9UujnodLxJdx1/Qwe29xMe4dK2DLlZ93lajqAK1lM\noZ+naqpjHD2rErZMqkskKZ80mk+rXE2ymEI/T912TRkzJpZoF0+GtJw8z6sqV5McoNDPU0WFBaxd\nGuMVlbBlxMbGoFxtmXbtSHZT6Oex7hK2RxtVwjaSUuVqTXxi7jRmTxsb9jgiV9Rv6JvZaDN7y8y2\nmdkuM/vz4P4VZrbZzHaa2Y/MrKiPx3ea2dbg8tRw/wWkb1Wl41g+Zyr1iaRK2EbQm/uOceDYOWqq\nVaEs2S+dLf2LwAp3XwQsBu42s08CPwIedPebgA+BL/fx+PPuvji4rB6WqSVtG+KV7D96jrf2HQt7\nlLxVl0gyYbTK1SQ39Bv6nnImuFkcXDqBdnd/L7j/JWDtyIwoQ3HPRyVs2sUzEk5duMRzO1pYvWgW\no4tVribZL619+mZWaGZbgVZSAf8WUGRm8WCRdUBfR7BGm1nCzN4wszV9PP9DwTKJtra2Af4V5EpS\nJWyzeG6HSthGwlNbD3Kxo4sN+uBzyRFphb67d7r7YiAGLAduBB4E/sbM3gJOk9r6781V7h4Hvgg8\nbGZX9/L8P3T3uLvHy8p0jvNwq4nHOH+pk2dUwjbs6hNJ5s+cwIIKlatJbhjQ2TvufgLYBNzt7r9y\n90+7+3LgVeC9Ph7THPy5F3gFWDKkiWXAFldO5toZ46nVB6cPq3cOnWJb00lq4ipXk9yRztk7ZWY2\nObg+Bvgs8I6ZTQ/uKwH+GPhBL4+dEnwdMysFbgXeHr7xJR1mRk28kq3JE7ynErZhU9uQZFRhgcrV\nJKeks6VfDmwys+1AA/CSuz8DfNPMdgPbgafd/WUAM4ub2SPBY68HEma2jdRvCH/p7gr9ENy/pILi\nQqNOW/vD4mJHJ08E5WpTVK4mOaTXc+t7cvft9LJLxt2/CXyzl/sTwNeC678EFgx9TBmqad0lbFua\n+dbd8xlVpPflDcXP3m7l+LlL1OgAruQY/eRHSE28kmNn23n5ncNhj5Lz6hJJZk0azafmlYY9isiA\nKPQj5LZry5g5cbQO6A7RwRN8W3GKAAAFkElEQVTnefV9latJblLoR0hhgbF2WQU/f6+NQydVwjZY\nGxubcId1KleTHKTQj5j1y1IlbBs36x26g9HV5dQ1Jvnk1SpXk9yk0I+YqtJx3DxnKnUqYRuUN/Yd\nJXnsvD4dS3KWQj+CNlRX8uHRc7ypErYBq2tIlavdfdPMsEcRGRSFfgTdc1M5E0qK9KlaA3Ty/CWe\n33mILyxWuZrkLoV+BI0ZVch9i1MlbKdUwpa2p7YF5Wrx2WGPIjJo/b45S/JTTbySH795gHsefo2x\no7TVmo5DJy8wf+YEbqqYGPYoIoOm0I+oRbFJ/P7tV7P/6NmwR8kZ18wYzxeXX6VyNclpCv2IMjO+\ndff8sMcQkQzTPn0RkQhR6IuIRIhCX0QkQhT6IiIRotAXEYkQhb6ISIQo9EVEIkShLyISIZZt9bpm\n1gZ8OISnKAWODNM4w0lzDYzmGhjNNTD5ONdV7l7W30JZF/pDZWYJd4+HPcflNNfAaK6B0VwDE+W5\ntHtHRCRCFPoiIhGSj6H/w7AH6IPmGhjNNTCaa2AiO1fe7dMXEZG+5eOWvoiI9EGhLyISITkZ+mb2\nD2bWamY7+/i6mdn3zWyPmW03s6VZMtftZnbSzLYGlz/J0FyVZrbJzN42s11m9vVelsn4Oktzroyv\nMzMbbWZvmdm2YK4/72WZEjOrDdbXm2ZWlSVzfcXM2nqsr6+N9Fw9XrvQzLaY2TO9fC3j6yuNmcJc\nV/vNbEfwuolevj5yP4/unnMX4DZgKbCzj6/fCzwPGHAL8GaWzHU78EwI66scWBpcnwC8B9wQ9jpL\nc66Mr7NgHYwPrhcDbwK3XLbM7wM/CK4/CNRmyVxfAf53pr/Hgtf+BvDj3v69wlhfacwU5rraD5Re\n4esj9vOYk1v67v4qcOwKi3wB+L+e8gYw2czKs2CuULh7i7tvDq6fBnYDFZctlvF1luZcGResgzPB\nzeLgcvkZD18AfhRcfxS400b4w3PTnCsUZhYDVgKP9LFIxtdXGjNlsxH7eczJ0E9DBZDscbuJLAiT\nwCeCX8+fN7MbM/3iwa/VS0htJfYU6jq7wlwQwjoLdgtsBVqBl9y9z/Xl7h3ASWBaFswFsDbYJfCo\nmVWO9EyBh4FvAV19fD2M9dXfTBDOuoLUf9Y/NbNGM3uol6+P2M9jvoZ+ttpMqh9jEfC/gCcy+eJm\nNh7YCPwndz+Vyde+kn7mCmWduXunuy8GYsByM7spE6/bnzTmehqocveFwEt8vHU9YsxsFdDq7o0j\n/VrpSnOmjK+rHj7l7kuBe4A/MLPbMvXC+Rr6zUDP/7VjwX2hcvdT3b+eu/tzQLGZlWbitc2smFSw\n/ou7P9bLIqGss/7mCnOdBa95AtgE3H3Zlz5aX2ZWBEwCjoY9l7sfdfeLwc1HgGUZGOdWYLWZ7Qd+\nAqwws3++bJlMr69+ZwppXXW/dnPwZyvwOLD8skVG7OcxX0P/KeB3giPgtwAn3b0l7KHMbGb3fkwz\nW05q/Y94UASv+ffAbnf/6z4Wy/g6S2euMNaZmZWZ2eTg+hjgs8A7ly32FPDl4Po64GUPjsCFOddl\n+31XkzpOMqLc/dvuHnP3KlIHaV9299+6bLGMrq90ZgpjXQWvO87MJnRfBz4HXH7G34j9PBYNx5Nk\nmpn9K6mzOkrNrAn4U1IHtXD3HwDPkTr6vQc4B/z7LJlrHfB7ZtYBnAceHOmgCNwK/DawI9gfDPBf\ngdk9ZgtjnaUzVxjrrBz4kZkVkvpPps7dnzGz7wAJd3+K1H9W/8/M9pA6eP/gCM+U7lx/ZGargY5g\nrq9kYK5eZcH66m+msNbVDODxYFumCPixu79gZr8LI//zqBoGEZEIydfdOyIi0guFvohIhCj0RUQi\nRKEvIhIhCn0RkQhR6IuIRIhCX0QkQv4/GgWuFiZu2OsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f29032d2690>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 38.9 s, sys: 3.14 s, total: 42.1 s\n",
      "Wall time: 27.8 s\n"
     ]
    }
   ],
   "source": [
    "%time train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), 'model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load('model.pkl'))\n",
    "def test():\n",
    "    # Code for training the model\n",
    "    # Make sure to output a matplotlib graph of training losses\n",
    "    loss_arr = []\n",
    "    for i, (himage, limage, truth) in enumerate(test_loader):  \n",
    "\n",
    "        # Convert torch tensor to Variable\n",
    "        himage = Variable(himage)\n",
    "        limage = Variable(limage)\n",
    "        truth = Variable(truth)\n",
    "        if(use_gpu):\n",
    "            himage=himage.cuda()\n",
    "            limage=limage.cuda()\n",
    "            truth = truth.cuda()\n",
    "\n",
    "\n",
    "        outputs = model(himage, limage)\n",
    "        \n",
    "        writer.add_image('test/himage', himage)\n",
    "        writer.add_image('test/limage', limage)\n",
    "        writer.add_image('test/truth',truth)\n",
    "        writer.add_image('test/output', outputs.view(-1,3, y_size, x_size))\n",
    "        loss = criterion(outputs, truth)\n",
    "\n",
    "        loss_arr.append(loss.data[0])\n",
    "        writer.add_scalar('test/loss',loss.data[0])\n",
    "    \n",
    "    plt.plot( np.array(range(1,len(loss_arr)+1)), np.array(loss_arr))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value torch.Size([1, 1080, 1920])\n",
      "size:  (16, 1L, 1080L, 1920L) size_alt (1L, 1L, 1080L, 1920L)\n",
      "st  torch.Size([16, 1, 1080, 1920])\n",
      "st2  torch.Size([3, 16])\n",
      "stt  torch.Size([1920, 1080, 3, 1])\n",
      "stt2  torch.Size([1, 1920, 1080])\n",
      "value torch.Size([1, 1080, 1920])\n",
      "size:  (16, 1L, 1080L, 1920L) size_alt (1L, 1L, 1080L, 1920L)\n",
      "st  torch.Size([16, 1, 1080, 1920])\n",
      "st2  torch.Size([3, 16])\n",
      "stt  torch.Size([1920, 1080, 3, 1])\n",
      "stt2  torch.Size([1, 1920, 1080])\n",
      "value torch.Size([1, 1080, 1920])\n",
      "size:  (16, 1L, 1080L, 1920L) size_alt (1L, 1L, 1080L, 1920L)\n",
      "st  torch.Size([16, 1, 1080, 1920])\n",
      "st2  torch.Size([3, 16])\n",
      "stt  torch.Size([1920, 1080, 3, 1])\n",
      "stt2  torch.Size([1, 1920, 1080])\n",
      "torch.Size([1, 1080, 1920])\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "<class 'torch.cuda.FloatTensor'>\n",
      "<class 'torch.cuda.FloatTensor'>\n",
      "torch.Size([1, 1080, 1920, 2])\n",
      "**********************\n",
      "torch.Size([1, 1080, 1920, 8])\n",
      "torch.Size([1, 1080, 1920, 8])\n",
      "**********************\n",
      "torch.Size([1, 8, 3, 4, 16, 16])\n",
      "**********************\n",
      "torch.Size([1, 3, 4, 8, 1920, 1080])\n",
      "**********************\n",
      "$$$$$$$$$$$$$$$\n",
      "torch.Size([1, 3, 4, 1920, 1080])\n",
      "**********************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eeshangd/vision_env/lib/python2.7/site-packages/ipykernel_launcher.py:88: UserWarning: torch.range is deprecated in favor of torch.arange and will be removed in 0.3. Note that arange generates values in [start; end), not [start; end].\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCYD/B1ugrdYdb2a3AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2884535590>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.02 s, sys: 708 ms, total: 4.73 s\n",
      "Wall time: 2.98 s\n"
     ]
    }
   ],
   "source": [
    "%time test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# writer.export_scalars_to_json(\"./all_scalars.json\")\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vision_env",
   "language": "python",
   "name": "vision_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
