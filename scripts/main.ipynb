{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch implementation of Deep Bilteral Learning for Real Time Image Enhancement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, unicode_literals\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os, sys, glob\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "#Torch Imports\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as data\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1920, 16])\n",
      "torch.Size([16, 1080])\n",
      "torch.Size([16, 1080])\n"
     ]
    }
   ],
   "source": [
    "size = (256, 256)\n",
    "batch_size = 100\n",
    "learning_rate = 0.01\n",
    "num_epochs = 5\n",
    "\n",
    "# size of image, right now we are fixing this\n",
    "x_size = 1920\n",
    "y_size = 1080\n",
    "\n",
    "# Constants for slicing layer\n",
    "sx = 256.0/x_size\n",
    "sy = 256.0/y_size\n",
    "d  = 8\n",
    "\n",
    "# tensorboard for pytorch stuff\n",
    "writer = SummaryWriter()\n",
    "sample_rate = 44100\n",
    "freqs = [262, 294, 330, 349, 392, 440, 440, 440, 440, 440, 440]\n",
    "\n",
    "# calculating Fx and keeping as constant from precomputation\n",
    "x = range(x_size)\n",
    "x = np.asarray(x, dtype=np.float64)\n",
    "one = np.ones(x_size)\n",
    "x = np.stack((x, one), axis=1)\n",
    "x[:, 0] = x[:, 0]*sx\n",
    "i  = range(16)\n",
    "i  = np.asarray(i, dtype=np.float64)*-1\n",
    "one= np.ones(16)\n",
    "i = np.stack((one, i), axis=1).swapaxes(0, 1)\n",
    "fx = torch.from_numpy(x)\n",
    "i  = torch.from_numpy(i)\n",
    "fxi= torch.matmul(fx, i)\n",
    "fxi= torch.abs(fxi)\n",
    "print(fxi.shape)\n",
    "\n",
    "# calculating Fy and keeping as constant from precomputation\n",
    "y = range(y_size)\n",
    "y = np.asarray(y, dtype=np.float64)\n",
    "one = np.ones(y_size)\n",
    "y = np.stack((y, one), axis=1)\n",
    "y[:, 0] = y[:, 0]*sy\n",
    "y = y.swapaxes(0, 1)\n",
    "i  = range(16)\n",
    "i  = np.asarray(i, dtype=np.float64)*-1\n",
    "one= np.ones(16)\n",
    "i = np.stack((one, i), axis=1)\n",
    "fy = torch.from_numpy(y)\n",
    "i  = torch.from_numpy(i)\n",
    "fyi= torch.matmul(i, fy)\n",
    "fyi= torch.abs(fyi)\n",
    "print(fyi.shape)\n",
    "print(fyi.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, root_dir, train, transform=None):\n",
    "        #Init Function\n",
    "        super(Dataset, self).__init__()\n",
    "        self.root_dir = root_dir\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.full_res = []\n",
    "        self.low_res = []\n",
    "        self.truth = []\n",
    "        \n",
    "        if (train):\n",
    "            dir = self.root_dir + '/train/'\n",
    "        else :\n",
    "            dir = self.root_dir + '/test/'\n",
    "        \n",
    "#         for img_path in glob.glob (dir + '*.jpg'):\n",
    "        for img_path in glob.glob (dir +'input/' '*.tif'):\n",
    "            img_name = img_path.split('/')[-1]\n",
    "            print (img_name)\n",
    "#             himage = Image.open (img_path)\n",
    "#             himage = himage.resize (img_w,img_h)           \n",
    "#             limage = himage.resize (size)           \n",
    "#             output = Image.open(dir+'output/'+img_name)\n",
    "#             output = output.resize(img_w,img_h)\n",
    "\n",
    "            himage = cv2.imread (img_path)\n",
    "            print(himage.shape)\n",
    "            himage = cv2.resize (himage,(x_size, y_size))\n",
    "            print(himage.shape)\n",
    "            limage = cv2.resize (himage,size)\n",
    "            output = cv2.imread (dir+'output/'+img_name)\n",
    "            output = cv2.resize (output,(x_size, y_size))\n",
    "\n",
    "            \n",
    "            self.full_res.append (himage)\n",
    "            self.low_res.append (limage)\n",
    "            self.truth.append (output)\n",
    "\n",
    "    def __len__(self):\n",
    "        #Length function ?\n",
    "        return len(self.full_res)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #Accessor Function\n",
    "        if self.transform is None:\n",
    "            return (self.full_res[idx],self.low_res[idx])\n",
    "        else:\n",
    "            limg_transformed = self.transform(self.low_res[idx])\n",
    "            himg_transformed =  self.transform(self.full_res[idx])\n",
    "            truth_transformed = self.transform(self.truth[idx])\n",
    "            return (himg_transformed, limg_transformed, truth_transformed)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a0001-jmac_DSC1459.tif\n",
      "(2000, 3008, 3)\n",
      "(1080, 1920, 3)\n",
      "a0004-jmac_MG_1384.tif\n",
      "(2912, 4368, 3)\n",
      "(1080, 1920, 3)\n"
     ]
    }
   ],
   "source": [
    "composed_transform = transforms.Compose([transforms.ToTensor()])\n",
    "# train_dataset = Dataset (root_dir = '../data', train = True, transform = composed_transform)\n",
    "train_dataset = Dataset (root_dir = '../dataset', train = True, transform = composed_transform)\n",
    "# im, im2, im3 = train_dataset.__getitem__(0)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LocalFeatureNet (nn.Module):\n",
    "\n",
    "    def slicing_relu(self, F):\n",
    "        #Function similar to custom relu does the tau calculation\n",
    "        dimension = F.size()\n",
    "        F_temp = F.cuda()\n",
    "        F_temp = torch.ones(dimension).cuda().sub_(torch.abs(F_temp).float())\n",
    "        F_temp = F_temp.cpu().numpy()\n",
    "        F_temp[F_temp < 0] = 0\n",
    "        F_temp = torch.from_numpy(F_temp)\n",
    "        return F_temp\n",
    "    \n",
    "    def __init__(self):\n",
    "        super (LocalFeatureNet, self).__init__()\n",
    "        \n",
    "        self.relu  = nn.ReLU (inplace = True)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d (in_channels = 3,   out_channels = 8, kernel_size = 3, stride = 2, padding = 1)\n",
    "        self.conv2 = nn.Conv2d (in_channels = 8,  out_channels = 16,  kernel_size = 3, stride = 2, padding = 1)\n",
    "        self.conv3 = nn.Conv2d (in_channels = 16, out_channels = 32,  kernel_size= 3, stride = 2,padding = 1)\n",
    "        self.conv4 = nn.Conv2d (in_channels = 32, out_channels = 64,  kernel_size = 3, stride = 2, padding = 1)\n",
    "        \n",
    "        \n",
    "        self.localconv1 = nn.Conv2d (in_channels = 64, out_channels = 64,  kernel_size = 3, stride = 1, padding = 1)\n",
    "        self.localconv2 = nn.Conv2d (in_channels = 64, out_channels = 64,  kernel_size= 3, stride = 1, padding = 1)\n",
    "        \n",
    "        self.globalconv1 = nn.Conv2d (in_channels = 64, out_channels = 64,  kernel_size = 3, stride = 2,padding = 1)\n",
    "        self.globalconv2 = nn.Conv2d (in_channels = 64, out_channels = 64,  kernel_size = 3, stride = 2, padding = 1)\n",
    "        \n",
    "        self.globalfc1 = nn.Linear (1024, 256)\n",
    "        self.globalfc2 = nn.Linear (256, 128)\n",
    "        self.globalfc3 = nn.Linear (128, 64)\n",
    "        self.linear = nn.Conv2d(in_channels = 64, out_channels = 96,  kernel_size = 1, stride = 1)\n",
    "        \n",
    "        # Pixel Wise Network\n",
    "        self.pixelwise_bias = nn.Parameter (torch.rand(3, 1), requires_grad=True)\n",
    "        self.pixelwise_weight = nn.Parameter (torch.eye(3), requires_grad = True)\n",
    "        self.pixelwise_obias = nn.Parameter (torch.eye(1), requires_grad = True)\n",
    "        \n",
    "        self.relu_slopes = nn.Parameter(torch.rand(3,16), requires_grad = True)\n",
    "        self.relu_shifts = nn.Parameter(torch.rand(16,3), requires_grad = True)\n",
    "        \n",
    "        # Thing we do for upsampling\n",
    "        self.Fx = fxi\n",
    "        self.Fy = fyi\n",
    "        self.Fx = self.slicing_relu(self.Fx).cuda()\n",
    "        self.Fy = self.slicing_relu(self.Fy).cuda()\n",
    "        \n",
    "        \n",
    "    def custom_relu(self,channel,value):\n",
    "        print(\"value\",value.size())\n",
    "        size = (16,value.size()[0],value.size()[1],value.size()[2])\n",
    "        size_alt = (1L,value.size()[0],value.size()[1],value.size()[2])\n",
    "        print(\"size: \",size,\"size_alt\",size_alt)\n",
    "        value = value.expand(size)\n",
    "        \n",
    "#         print(self.relu_shifts[:,channel])\n",
    "        a = self.relu_shifts[:,channel].clone()\n",
    "        a = a.view(16,1,1,1)\n",
    "#         print(a.data,a)\n",
    "#         value = self.relu(value - a.repeat(size_alt))    \n",
    "# Upper line is not working for some damn reason so I hard coded it for now.\n",
    "#         print (\"here \", a.repeat(1,1,img_h,img_w).size())\n",
    "        value = self.relu(value - a.repeat(size[1],1, x_size, y_size))\n",
    "        print(\"st \", value.size())\n",
    "        print (\"st2 \", self.relu_slopes.size())\n",
    "        value = self.relu_slopes.matmul(value.view(x_size, y_size,16,-1))\n",
    "        print (\"stt \", value.size())\n",
    "        value = value.view(-1,3, x_size, y_size)\n",
    "        value = value[:,1,:,:]+value[:,2,:,:]+value[:,0,:,:]\n",
    "        print (\"stt2 \", value.size())\n",
    "        return value\n",
    "    \n",
    "    def upsample(self, grid, bilat):\n",
    "        \n",
    "        #### Making G\n",
    "        g = grid.cuda()\n",
    "        a = bilat.cuda()\n",
    "        print(type(bilat))\n",
    "        G = g.data\n",
    "        print(type(G))\n",
    "        G = d*G\n",
    "        one = torch.ones(G.shape).cuda()\n",
    "        print(type(one))\n",
    "        G = torch.stack([G, one], 3)\n",
    "        print(G.shape)\n",
    "        one = torch.ones(8)\n",
    "        K = torch.range(0, 7).cuda()* -1\n",
    "        one= torch.ones(8).cuda()\n",
    "        K = torch.stack((K, one), 1).permute(1, 0)\n",
    "        z = K[0]\n",
    "        K[0] = K[1]\n",
    "        K[1] = z\n",
    "#         G = torch.from_numpy(G)\n",
    "        F_gk = torch.matmul(G, K)\n",
    "        F_g = self.slicing_relu(F_gk)\n",
    "        print(\"**********************\")\n",
    "        print(F_gk.size())\n",
    "        print(F_g.size())\n",
    "        print(\"**********************\")\n",
    "        #########################################\n",
    "        \n",
    "        print(a.data.size())\n",
    "        A_temp = a.data.permute(0, 2, 3, 1, 4, 5)\n",
    "        F_prod = torch.matmul(self.Fx, A_temp)\n",
    "        F_prod = torch.matmul(F_prod, self.Fy)\n",
    "        print(\"**********************\")\n",
    "        print(F_prod.size())\n",
    "        print(\"**********************\")\n",
    "        F_prod_sum = F_prod.sum(3)\n",
    "        print(\"$$$$$$$$$$$$$$$\")\n",
    "        print(F_prod_sum.size())\n",
    "        print(\"**********************\")\n",
    "        a.data = F_prod_sum\n",
    "        return F_prod_sum\n",
    "        \n",
    "        \n",
    "        \n",
    "    def output(self, grid, inp):\n",
    "        out = torch.rand(inp.size()[0], y_size, x_size, 3).cuda()\n",
    "        out = 0 * out\n",
    "        grid = grid.view(-1, y_size, x_size, 12)\n",
    "        \n",
    "        for i in range(0,3):\n",
    "            out[:,:,:,i] = out[:,:,:,i].add_(grid[:,:,:,3+4*i])\n",
    "            for j in range(0,3):\n",
    "#                 print(grid[:,:,:,j+4*i].size())\n",
    "#                 print(inp[:,j,:,:].size())\n",
    "                a = torch.autograd.Variable(grid[:,:,:,j+4*i], requires_grad=False)\n",
    "#                 a = a.numpy()\n",
    "                b = inp[:,j,:,:]\n",
    "#                 b = b.numpy()\n",
    "                temp = a.data * b.data\n",
    "#                 print(temp.size())\n",
    "                out[:,:,:,i] = out[:,:,:,i] + temp\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def forward (self, h, l):\n",
    "        \n",
    "        x = self.relu (self.conv1 (l))\n",
    "        x = self.relu (self.conv2 (x))\n",
    "        x = self.relu (self.conv3 (x))\n",
    "        x = self.relu (self.conv4 (x))\n",
    "        y = self.localconv1 (x)\n",
    "        y = self.localconv2 (y)\n",
    "        z = self.globalconv1 (x)\n",
    "        z = self.globalconv2 (z)\n",
    "        z = self.globalfc1 (z.view(z.size()[0], -1))\n",
    "        z = self.globalfc2 (z)\n",
    "        z = self.globalfc3 (z)\n",
    "        fused = self.relu(z.view(-1,64,1,1)+y)\n",
    "        lin = self.linear(fused)\n",
    "        bilat = lin.view(-1, 8, 3, 4, 16, 16)\n",
    "#         print (bilat.size())\n",
    "\n",
    "# pixel wise network\n",
    "        for i in range(0,3):\n",
    "            a = self.pixelwise_weight[i,:].view(-1,3)\n",
    "            b = h.unsqueeze(0).view(3,-1)\n",
    "#             print(a.size(),b.size())\n",
    "            p = torch.mm(a,b)\n",
    "#             print(p.size())\n",
    "            p = p.view(h.size()[0],h.size()[2],h.size()[3]) + self.pixelwise_bias[i]\n",
    "#             print(p.size())\n",
    "#             p = torch.bmm(self.pixelwise_weight[i,:], h.unsqueeze(0).view(3,-1)).view(h.size()) + self.pixelwise_bias[i]\n",
    "#             print(i)\n",
    "            p += self.custom_relu(i,p)\n",
    "        p += self.pixelwise_obias\n",
    "#         print(p.size())\n",
    "        grid = p\n",
    "#         print(\"+++++++++++++++++++\")\n",
    "#         print(bilat.size())\n",
    "#         print(\"+++++++++++++++++++\")\n",
    "        print(grid.size())\n",
    "        bilat_new = self.upsample(grid, bilat)\n",
    "        #p is modified for tensorboard\n",
    "        p = p.view(p.size()[0],1,p.size()[1],p.size()[2])\n",
    "        writer.add_image('gridmap', p)\n",
    "        return self.output(bilat_new,h)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LocalFeatureNet() \n",
    "\n",
    "# Add code for using CUDA here if it is available\n",
    "use_gpu = False\n",
    "if(torch.cuda.is_available()):\n",
    "    use_gpu = True\n",
    "    model.cuda()\n",
    "\n",
    "# Loss function and optimizers\n",
    "criterion = torch.nn.MSELoss()# Define MSE loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)# Use Adam optimizer, use learning_rate hyper parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Code for training the model\n",
    "    # Make sure to output a matplotlib graph of training losses\n",
    "    loss_arr = []\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (himage, limage, truth) in enumerate(train_loader):  \n",
    "            \n",
    "            # Convert torch tensor to Variable\n",
    "            himage = Variable(himage)\n",
    "            limage = Variable(limage)\n",
    "            truth = Variable(truth)\n",
    "            if(use_gpu):\n",
    "                himage=himage.cuda()\n",
    "                limage=limage.cuda()\n",
    "                truth = truth.cuda()\n",
    "            \n",
    "            # Forward + Backward + Optimize\n",
    "            optimizer.zero_grad()  # zero the gradient buffer\n",
    "            outputs = model(himage, limage)\n",
    "            writer.add_image('himage', himage)\n",
    "            writer.add_image('limage', limage)\n",
    "            writer.add_image('truth',truth)\n",
    "            writer.add_image('output', outputs.view(-1,3, y_size, x_size))\n",
    "            loss = criterion(outputs, truth)\n",
    "            \n",
    "            model.pixelwise_obias.backward()\n",
    "            \n",
    "            model.pixelwise_bias[0,0].backward()\n",
    "            model.pixelwise_bias[1,0].backward()\n",
    "            model.pixelwise_bias[2,0].backward()\n",
    "            \n",
    "            model.pixelwise_weight[0,0].backward()\n",
    "            model.pixelwise_weight[0,1].backward()\n",
    "            model.pixelwise_weight[0,2].backward()\n",
    "            model.pixelwise_weight[1,0].backward()\n",
    "            model.pixelwise_weight[1,1].backward()\n",
    "            model.pixelwise_weight[1,2].backward()\n",
    "            model.pixelwise_weight[2,0].backward()\n",
    "            model.pixelwise_weight[2,1].backward()\n",
    "            model.pixelwise_weight[2,2].backward()\n",
    "            \n",
    "            for i in range(3):\n",
    "                for j in range(16):\n",
    "                    model.relu_slopes[i,j].backward()\n",
    "                    model.relu_shifts[j,i].backward()\n",
    "                    \n",
    "#             loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_arr.append(loss.data[0])\n",
    "            if (i+1) % batch_size == 0:       \n",
    "                print ('Epoch [%d/%d], Step [%d/%d], Loss: %.4f' \n",
    "                       %(epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.data[0]))\n",
    "    \n",
    "    plt.plot( np.array(range(1,len(loss_arr)+1)), np.array(loss_arr))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value torch.Size([2, 1080, 1920])\n",
      "size:  (16, 2L, 1080L, 1920L) size_alt (1L, 2L, 1080L, 1920L)\n",
      "st  torch.Size([16, 2, 1080, 1920])\n",
      "st2  torch.Size([3, 16])\n",
      "stt  torch.Size([1920, 1080, 3, 2])\n",
      "stt2  torch.Size([2, 1920, 1080])\n",
      "value torch.Size([2, 1080, 1920])\n",
      "size:  (16, 2L, 1080L, 1920L) size_alt (1L, 2L, 1080L, 1920L)\n",
      "st  torch.Size([16, 2, 1080, 1920])\n",
      "st2  torch.Size([3, 16])\n",
      "stt  torch.Size([1920, 1080, 3, 2])\n",
      "stt2  torch.Size([2, 1920, 1080])\n",
      "value torch.Size([2, 1080, 1920])\n",
      "size:  (16, 2L, 1080L, 1920L) size_alt (1L, 2L, 1080L, 1920L)\n",
      "st  torch.Size([16, 2, 1080, 1920])\n",
      "st2  torch.Size([3, 16])\n",
      "stt  torch.Size([1920, 1080, 3, 2])\n",
      "stt2  torch.Size([2, 1920, 1080])\n",
      "torch.Size([2, 1080, 1920])\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "<class 'torch.cuda.FloatTensor'>\n",
      "<class 'torch.cuda.FloatTensor'>\n",
      "torch.Size([2, 1080, 1920, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eeshangd/vision_env/lib/python2.7/site-packages/ipykernel_launcher.py:88: UserWarning: torch.range is deprecated in favor of torch.arange and will be removed in 0.3. Note that arange generates values in [start; end), not [start; end].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********************\n",
      "torch.Size([2, 1080, 1920, 8])\n",
      "torch.Size([2, 1080, 1920, 8])\n",
      "**********************\n",
      "torch.Size([2, 8, 3, 4, 16, 16])\n",
      "**********************\n",
      "torch.Size([2, 3, 4, 8, 1920, 1080])\n",
      "**********************\n",
      "$$$$$$$$$$$$$$$\n",
      "torch.Size([2, 3, 4, 1920, 1080])\n",
      "**********************\n",
      "value torch.Size([2, 1080, 1920])\n",
      "size:  (16, 2L, 1080L, 1920L) size_alt (1L, 2L, 1080L, 1920L)\n",
      "st  torch.Size([16, 2, 1080, 1920])\n",
      "st2  torch.Size([3, 16])\n",
      "stt  torch.Size([1920, 1080, 3, 2])\n",
      "stt2  torch.Size([2, 1920, 1080])\n",
      "value torch.Size([2, 1080, 1920])\n",
      "size:  (16, 2L, 1080L, 1920L) size_alt (1L, 2L, 1080L, 1920L)\n",
      "st  torch.Size([16, 2, 1080, 1920])\n",
      "st2  torch.Size([3, 16])\n",
      "stt  torch.Size([1920, 1080, 3, 2])\n",
      "stt2  torch.Size([2, 1920, 1080])\n",
      "value torch.Size([2, 1080, 1920])\n",
      "size:  (16, 2L, 1080L, 1920L) size_alt (1L, 2L, 1080L, 1920L)\n",
      "st  torch.Size([16, 2, 1080, 1920])\n",
      "st2  torch.Size([3, 16])\n",
      "stt  torch.Size([1920, 1080, 3, 2])\n",
      "stt2  torch.Size([2, 1920, 1080])\n",
      "torch.Size([2, 1080, 1920])\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "<class 'torch.cuda.FloatTensor'>\n",
      "<class 'torch.cuda.FloatTensor'>\n",
      "torch.Size([2, 1080, 1920, 2])\n",
      "**********************\n",
      "torch.Size([2, 1080, 1920, 8])\n",
      "torch.Size([2, 1080, 1920, 8])\n",
      "**********************\n",
      "torch.Size([2, 8, 3, 4, 16, 16])\n",
      "**********************\n",
      "torch.Size([2, 3, 4, 8, 1920, 1080])\n",
      "**********************\n",
      "$$$$$$$$$$$$$$$\n",
      "torch.Size([2, 3, 4, 1920, 1080])\n",
      "**********************\n",
      "value torch.Size([2, 1080, 1920])\n",
      "size:  (16, 2L, 1080L, 1920L) size_alt (1L, 2L, 1080L, 1920L)\n",
      "st  torch.Size([16, 2, 1080, 1920])\n",
      "st2  torch.Size([3, 16])\n",
      "stt  torch.Size([1920, 1080, 3, 2])\n",
      "stt2  torch.Size([2, 1920, 1080])\n",
      "value torch.Size([2, 1080, 1920])\n",
      "size:  (16, 2L, 1080L, 1920L) size_alt (1L, 2L, 1080L, 1920L)\n",
      "st  torch.Size([16, 2, 1080, 1920])\n",
      "st2  torch.Size([3, 16])\n",
      "stt  torch.Size([1920, 1080, 3, 2])\n",
      "stt2  torch.Size([2, 1920, 1080])\n",
      "value torch.Size([2, 1080, 1920])\n",
      "size:  (16, 2L, 1080L, 1920L) size_alt (1L, 2L, 1080L, 1920L)\n",
      "st  torch.Size([16, 2, 1080, 1920])\n",
      "st2  torch.Size([3, 16])\n",
      "stt  torch.Size([1920, 1080, 3, 2])\n",
      "stt2  torch.Size([2, 1920, 1080])\n",
      "torch.Size([2, 1080, 1920])\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "<class 'torch.cuda.FloatTensor'>\n",
      "<class 'torch.cuda.FloatTensor'>\n",
      "torch.Size([2, 1080, 1920, 2])\n",
      "**********************\n",
      "torch.Size([2, 1080, 1920, 8])\n",
      "torch.Size([2, 1080, 1920, 8])\n",
      "**********************\n",
      "torch.Size([2, 8, 3, 4, 16, 16])\n",
      "**********************\n",
      "torch.Size([2, 3, 4, 8, 1920, 1080])\n",
      "**********************\n",
      "$$$$$$$$$$$$$$$\n",
      "torch.Size([2, 3, 4, 1920, 1080])\n",
      "**********************\n",
      "value torch.Size([2, 1080, 1920])\n",
      "size:  (16, 2L, 1080L, 1920L) size_alt (1L, 2L, 1080L, 1920L)\n",
      "st  torch.Size([16, 2, 1080, 1920])\n",
      "st2  torch.Size([3, 16])\n",
      "stt  torch.Size([1920, 1080, 3, 2])\n",
      "stt2  torch.Size([2, 1920, 1080])\n",
      "value torch.Size([2, 1080, 1920])\n",
      "size:  (16, 2L, 1080L, 1920L) size_alt (1L, 2L, 1080L, 1920L)\n",
      "st  torch.Size([16, 2, 1080, 1920])\n",
      "st2  torch.Size([3, 16])\n",
      "stt  torch.Size([1920, 1080, 3, 2])\n",
      "stt2  torch.Size([2, 1920, 1080])\n",
      "value torch.Size([2, 1080, 1920])\n",
      "size:  (16, 2L, 1080L, 1920L) size_alt (1L, 2L, 1080L, 1920L)\n",
      "st  torch.Size([16, 2, 1080, 1920])\n",
      "st2  torch.Size([3, 16])\n",
      "stt  torch.Size([1920, 1080, 3, 2])\n",
      "stt2  torch.Size([2, 1920, 1080])\n",
      "torch.Size([2, 1080, 1920])\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "<class 'torch.cuda.FloatTensor'>\n",
      "<class 'torch.cuda.FloatTensor'>\n",
      "torch.Size([2, 1080, 1920, 2])\n",
      "**********************\n",
      "torch.Size([2, 1080, 1920, 8])\n",
      "torch.Size([2, 1080, 1920, 8])\n",
      "**********************\n",
      "torch.Size([2, 8, 3, 4, 16, 16])\n",
      "**********************\n",
      "torch.Size([2, 3, 4, 8, 1920, 1080])\n",
      "**********************\n",
      "$$$$$$$$$$$$$$$\n",
      "torch.Size([2, 3, 4, 1920, 1080])\n",
      "**********************\n",
      "value torch.Size([2, 1080, 1920])\n",
      "size:  (16, 2L, 1080L, 1920L) size_alt (1L, 2L, 1080L, 1920L)\n",
      "st  torch.Size([16, 2, 1080, 1920])\n",
      "st2  torch.Size([3, 16])\n",
      "stt  torch.Size([1920, 1080, 3, 2])\n",
      "stt2  torch.Size([2, 1920, 1080])\n",
      "value torch.Size([2, 1080, 1920])\n",
      "size:  (16, 2L, 1080L, 1920L) size_alt (1L, 2L, 1080L, 1920L)\n",
      "st  torch.Size([16, 2, 1080, 1920])\n",
      "st2  torch.Size([3, 16])\n",
      "stt  torch.Size([1920, 1080, 3, 2])\n",
      "stt2  torch.Size([2, 1920, 1080])\n",
      "value torch.Size([2, 1080, 1920])\n",
      "size:  (16, 2L, 1080L, 1920L) size_alt (1L, 2L, 1080L, 1920L)\n",
      "st  torch.Size([16, 2, 1080, 1920])\n",
      "st2  torch.Size([3, 16])\n",
      "stt  torch.Size([1920, 1080, 3, 2])\n",
      "stt2  torch.Size([2, 1920, 1080])\n",
      "torch.Size([2, 1080, 1920])\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "<class 'torch.cuda.FloatTensor'>\n",
      "<class 'torch.cuda.FloatTensor'>\n",
      "torch.Size([2, 1080, 1920, 2])\n",
      "**********************\n",
      "torch.Size([2, 1080, 1920, 8])\n",
      "torch.Size([2, 1080, 1920, 8])\n",
      "**********************\n",
      "torch.Size([2, 8, 3, 4, 16, 16])\n",
      "**********************\n",
      "torch.Size([2, 3, 4, 8, 1920, 1080])\n",
      "**********************\n",
      "$$$$$$$$$$$$$$$\n",
      "torch.Size([2, 3, 4, 1920, 1080])\n",
      "**********************\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIEkC4H8AGvT6dpyvaQAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2895836250>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 38.3 s, sys: 2.95 s, total: 41.3 s\n",
      "Wall time: 27.8 s\n"
     ]
    }
   ],
   "source": [
    "%time train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# writer.export_scalars_to_json(\"./all_scalars.json\")\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vision_env",
   "language": "python",
   "name": "vision_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
