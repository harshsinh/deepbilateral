{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch implementation of Deep Bilteral Learning for Real Time Image Enhancement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, unicode_literals\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import os, sys, glob\n",
    "import matplotlib.pyplot as plt\n",
    "#Torch Imports\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as data\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = (256, 256)\n",
    "batch_size = 10\n",
    "learning_rate = 0.01\n",
    "num_epochs = 1\n",
    "img_w = 400\n",
    "img_h = 300\n",
    "\n",
    "use_gpu = True\n",
    "\n",
    "writer = SummaryWriter()\n",
    "sample_rate = 44100\n",
    "freqs = [262, 294, 330, 349, 392, 440, 440, 440, 440, 440, 440]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, root_dir, train, transform=None):\n",
    "        #Init Function\n",
    "        super(Dataset, self).__init__()\n",
    "        self.root_dir = root_dir\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.full_res = []\n",
    "        self.low_res = []\n",
    "        self.truth = []\n",
    "        \n",
    "        if (train):\n",
    "            dir = self.root_dir + '/train/'\n",
    "        else :\n",
    "            dir = self.root_dir + '/test/'\n",
    "        \n",
    "#         for img_path in glob.glob (dir + '*.jpg'):\n",
    "        imcount = 0\n",
    "        for img_path in glob.glob (dir +'input/' '*.tif'):\n",
    "#             if(imcount > len(glob.glob(dir + 'input/' '*.tif'))):\n",
    "#                 continue\n",
    "            img_name = img_path.split('/')[-1]\n",
    "            print (img_name)\n",
    "#             himage = Image.open (img_path)\n",
    "#             himage = himage.resize (img_w,img_h)           \n",
    "#             limage = himage.resize (size)           \n",
    "#             output = Image.open(dir+'output/'+img_name)\n",
    "#             output = output.resize(img_w,img_h)\n",
    "\n",
    "            himage = cv2.imread (img_path)\n",
    "            print(himage.shape)\n",
    "            himage = cv2.resize (himage,(img_w,img_h))\n",
    "            print(himage.shape)\n",
    "            limage = cv2.resize (himage,size)\n",
    "            output = cv2.imread (dir+'output/'+img_name)\n",
    "            output = cv2.resize (output,(img_w,img_h))\n",
    "\n",
    "            \n",
    "            self.full_res.append (himage)\n",
    "            self.low_res.append (limage)\n",
    "            self.truth.append (output)\n",
    "            \n",
    "            imcount = imcount + 1\n",
    "\n",
    "    def __len__(self):\n",
    "        #Length function ?\n",
    "        return len(self.full_res)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #Accessor Function\n",
    "        if self.transform is None:\n",
    "            return (self.full_res[idx],self.low_res[idx])\n",
    "        else:\n",
    "            limg_transformed = self.transform(self.low_res[idx])\n",
    "            himg_transformed =  self.transform(self.full_res[idx])\n",
    "            truth_transformed = self.transform(self.truth[idx])\n",
    "            return (himg_transformed, limg_transformed, truth_transformed)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a0089-jn_20080509_245.tif\n",
      "(3504, 2336, 3)\n",
      "(300, 400, 3)\n",
      "a0017-050710_031618__MG_3496.tif\n",
      "(2336, 3504, 3)\n",
      "(300, 400, 3)\n",
      "a0029-jmac_DSC3974.tif\n",
      "(3008, 2000, 3)\n",
      "(300, 400, 3)\n",
      "a0076-jmac_MG_5736.tif\n",
      "(2912, 4368, 3)\n",
      "(300, 400, 3)\n",
      "a0031-WP_CRW_0736.tif\n",
      "(3072, 2048, 3)\n",
      "(300, 400, 3)\n",
      "a0070-IMG_4327.tif\n",
      "(4272, 2848, 3)\n",
      "(300, 400, 3)\n",
      "a0006-IMG_2787.tif\n",
      "(2848, 4272, 3)\n",
      "(300, 400, 3)\n",
      "a0043-07-11-27-at-12h09m46s-_MG_7307.tif\n",
      "(2912, 4368, 3)\n",
      "(300, 400, 3)\n",
      "a0098-kme_187.tif\n",
      "(2336, 3504, 3)\n",
      "(300, 400, 3)\n",
      "a0093-MB_070908_038.tif\n",
      "(4368, 2912, 3)\n",
      "(300, 400, 3)\n",
      "a0082-IMG_4578.tif\n",
      "(2848, 4272, 3)\n",
      "(300, 400, 3)\n",
      "a0025-kme_298.tif\n",
      "(2040, 3060, 3)\n",
      "(300, 400, 3)\n",
      "a0036-jn_2007_05_05__183.tif\n",
      "(2336, 3504, 3)\n",
      "(300, 400, 3)\n",
      "a0004-jmac_MG_1384.tif\n",
      "(2912, 4368, 3)\n",
      "(300, 400, 3)\n",
      "a0095-IMG_9199.tif\n",
      "(2336, 3504, 3)\n",
      "(300, 400, 3)\n",
      "a0072-IMG_5048.tif\n",
      "(3888, 2592, 3)\n",
      "(300, 400, 3)\n",
      "a0087-kme_155.tif\n",
      "(2336, 3504, 3)\n",
      "(300, 400, 3)\n",
      "a0038-MB_070908_135.tif\n",
      "(4368, 2912, 3)\n",
      "(300, 400, 3)\n",
      "a0028-_DSC0032.tif\n",
      "(3008, 2000, 3)\n",
      "(300, 400, 3)\n",
      "a0085-kme_215.tif\n",
      "(2336, 3504, 3)\n",
      "(300, 400, 3)\n",
      "a0092-jmac_MG_7673.tif\n",
      "(2912, 4368, 3)\n",
      "(300, 400, 3)\n",
      "a0024-_DSC8932.tif\n",
      "(2832, 4256, 3)\n",
      "(300, 400, 3)\n",
      "a0039-jmac_DSC3406.tif\n",
      "(3008, 2000, 3)\n",
      "(300, 400, 3)\n",
      "a0065-_DSC6405.tif\n",
      "(2832, 4256, 3)\n",
      "(300, 400, 3)\n",
      "a0078-IMG_0326.tif\n",
      "(3888, 2592, 3)\n",
      "(300, 400, 3)\n",
      "a0015-DSC_0081.tif\n",
      "(3008, 2000, 3)\n",
      "(300, 400, 3)\n",
      "a0053-kme_103.tif\n",
      "(3888, 2592, 3)\n",
      "(300, 400, 3)\n",
      "a0021-07-11-28-at-09h22m57s-_MG_7427.tif\n",
      "(2912, 4368, 3)\n",
      "(300, 400, 3)\n",
      "a0033-KE_-2590.tif\n",
      "(2000, 3008, 3)\n",
      "(300, 400, 3)\n",
      "a0046-dgw_101.tif\n",
      "(2832, 4256, 3)\n",
      "(300, 400, 3)\n",
      "a0010-jmac_MG_4807.tif\n",
      "(2336, 3504, 3)\n",
      "(300, 400, 3)\n",
      "a0064-_DSC7889.tif\n",
      "(2832, 4256, 3)\n",
      "(300, 400, 3)\n",
      "a0080-kme_544.tif\n",
      "(4416, 3312, 3)\n",
      "(300, 400, 3)\n",
      "a0074-WP_CRW_0343.tif\n",
      "(3072, 2048, 3)\n",
      "(300, 400, 3)\n",
      "a0026-kme_391.tif\n",
      "(2336, 3504, 3)\n",
      "(300, 400, 3)\n",
      "a0084-_MG_1610.tif\n",
      "(4368, 2912, 3)\n",
      "(300, 400, 3)\n",
      "a0060-jmac_DSC3171.tif\n",
      "(2000, 3008, 3)\n",
      "(300, 400, 3)\n",
      "a0002-dgw_005.tif\n",
      "(4256, 2832, 3)\n",
      "(300, 400, 3)\n",
      "a0067-kme_213.tif\n",
      "(3504, 2336, 3)\n",
      "(300, 400, 3)\n",
      "a0012-kme_143.tif\n",
      "(3504, 2336, 3)\n",
      "(300, 400, 3)\n",
      "a0063-IMG_4185.tif\n",
      "(2848, 4272, 3)\n",
      "(300, 400, 3)\n",
      "a0020-jmac_MG_6225.tif\n",
      "(2912, 4368, 3)\n",
      "(300, 400, 3)\n",
      "a0057-kme_406.tif\n",
      "(3504, 2336, 3)\n",
      "(300, 400, 3)\n",
      "a0055-050729_194412__I2E5282.tif\n",
      "(2336, 3504, 3)\n",
      "(300, 400, 3)\n",
      "a0041-IMG_4972.tif\n",
      "(2848, 4272, 3)\n",
      "(300, 400, 3)\n",
      "a0090-LS_060715_8754.tif\n",
      "(3328, 4992, 3)\n",
      "(300, 400, 3)\n",
      "a0023-07-06-02-at-15h06m48-s_MG_1489.tif\n",
      "(2912, 4368, 3)\n",
      "(300, 400, 3)\n",
      "a0003-NKIM_MG_8178.tif\n",
      "(3888, 2592, 3)\n",
      "(300, 400, 3)\n",
      "a0027-IMG_3964.tif\n",
      "(2848, 4272, 3)\n",
      "(300, 400, 3)\n",
      "a0061-jn_20080802_0003.tif\n",
      "(2336, 3504, 3)\n",
      "(300, 400, 3)\n",
      "a0077-20080627_at_14h31m24__MG_0714.tif\n",
      "(2912, 4368, 3)\n",
      "(300, 400, 3)\n",
      "a0068-LS051026_day_1_arive53.tif\n",
      "(4992, 3328, 3)\n",
      "(300, 400, 3)\n",
      "a0056-LSYD4O0815.tif\n",
      "(3328, 4992, 3)\n",
      "(300, 400, 3)\n",
      "a0069-kme_056.tif\n",
      "(3504, 2336, 3)\n",
      "(300, 400, 3)\n",
      "a0014-WP_CRW_6320.tif\n",
      "(2048, 3072, 3)\n",
      "(300, 400, 3)\n",
      "a0094-CRW_2714.tif\n",
      "(3072, 2048, 3)\n",
      "(300, 400, 3)\n",
      "a0083-jmac_MG_0082.tif\n",
      "(2912, 4368, 3)\n",
      "(300, 400, 3)\n",
      "a0018-kme_234.tif\n",
      "(2336, 3504, 3)\n",
      "(300, 400, 3)\n",
      "a0016-jmac_MG_0795.tif\n",
      "(2912, 4368, 3)\n",
      "(300, 400, 3)\n",
      "a0035-dgw_048.tif\n",
      "(2832, 4256, 3)\n",
      "(300, 400, 3)\n",
      "a0091-jmac_MG_4959.tif\n",
      "(2912, 4368, 3)\n",
      "(300, 400, 3)\n",
      "a0086-WP_CRW_3706.tif\n",
      "(3072, 2048, 3)\n",
      "(300, 400, 3)\n",
      "a0059-JI2E5556.tif\n",
      "(2336, 3504, 3)\n",
      "(300, 400, 3)\n",
      "a0045-20050320_081337__E6B1177.tif\n",
      "(2336, 3504, 3)\n",
      "(300, 400, 3)\n",
      "a0005-jn_2007_05_10__564.tif\n",
      "(3504, 2336, 3)\n",
      "(300, 400, 3)\n",
      "a0009-kme_372.tif\n",
      "(4256, 2848, 3)\n",
      "(300, 400, 3)\n",
      "a0013-MB_20030906_001.tif\n",
      "(3072, 2048, 3)\n",
      "(300, 400, 3)\n",
      "a0100-AlexWed07-9691.tif\n",
      "(3888, 2592, 3)\n",
      "(300, 400, 3)\n",
      "a0079-IMG_4099.tif\n",
      "(2848, 4272, 3)\n",
      "(300, 400, 3)\n",
      "a0030-_MG_7844.tif\n",
      "(2912, 4368, 3)\n",
      "(300, 400, 3)\n",
      "a0034-LSYD4O2202.tif\n",
      "(3328, 4992, 3)\n",
      "(300, 400, 3)\n",
      "a0011-DSC_0082.tif\n",
      "(3008, 2000, 3)\n",
      "(300, 400, 3)\n",
      "a0022-IMG_2380.tif\n",
      "(2848, 4272, 3)\n",
      "(300, 400, 3)\n",
      "a0081-kme_106.tif\n",
      "(2592, 3888, 3)\n",
      "(300, 400, 3)\n",
      "a0032-jmac_MG_0266.tif\n",
      "(2912, 4368, 3)\n",
      "(300, 400, 3)\n",
      "a0096-_DGW6249.tif\n",
      "(2832, 4256, 3)\n",
      "(300, 400, 3)\n",
      "a0042-060813_155838__MG_6361.tif\n",
      "(4368, 2912, 3)\n",
      "(300, 400, 3)\n",
      "a0066-Ja_Pe-65.tif\n",
      "(2376, 3568, 3)\n",
      "(300, 400, 3)\n",
      "a0097-JI2E5089.tif\n",
      "(2336, 3504, 3)\n",
      "(300, 400, 3)\n",
      "a0044-JI2E0511.tif\n",
      "(2336, 3504, 3)\n",
      "(300, 400, 3)\n",
      "a0047-07-11-18-at-00h05m40s-_MG_4882.tif\n",
      "(4368, 2912, 3)\n",
      "(300, 400, 3)\n",
      "a0073-IMG_2593.tif\n",
      "(2848, 4272, 3)\n",
      "(300, 400, 3)\n",
      "a0051-WP_CRW_3461.tif\n",
      "(3072, 2048, 3)\n",
      "(300, 400, 3)\n",
      "a0054-kme_097.tif\n",
      "(2592, 3888, 3)\n",
      "(300, 400, 3)\n",
      "a0007-IMG_2480.tif\n",
      "(4272, 2848, 3)\n",
      "(300, 400, 3)\n",
      "a0049-_I2E5952.tif\n",
      "(3504, 2336, 3)\n",
      "(300, 400, 3)\n",
      "a0050-_DSC0006.tif\n",
      "(2000, 3008, 3)\n",
      "(300, 400, 3)\n",
      "a0071-kme_490.tif\n",
      "(3312, 4416, 3)\n",
      "(300, 400, 3)\n",
      "a0040-_DSC5693.tif\n",
      "(4256, 2832, 3)\n",
      "(300, 400, 3)\n",
      "a0019-jmac_MG_0653.tif\n",
      "(4368, 2912, 3)\n",
      "(300, 400, 3)\n",
      "a0037-jmacAlgarve_Sagres_07.tif\n",
      "(2848, 4256, 3)\n",
      "(300, 400, 3)\n",
      "a0048-kme_066.tif\n",
      "(2592, 3888, 3)\n",
      "(300, 400, 3)\n",
      "a0075-jn_20080814_New_York_City_042.tif\n",
      "(2336, 3504, 3)\n",
      "(300, 400, 3)\n",
      "a0008-WP_CRW_3959.tif\n",
      "(3072, 2048, 3)\n",
      "(300, 400, 3)\n",
      "a0099-kme_264.tif\n",
      "(2000, 3008, 3)\n",
      "(300, 400, 3)\n",
      "a0088-_DGW6376.tif\n",
      "(2832, 4256, 3)\n",
      "(300, 400, 3)\n",
      "a0062-LSCRW_0035.tif\n",
      "(2304, 3072, 3)\n",
      "(300, 400, 3)\n",
      "a0001-jmac_DSC1459.tif\n",
      "(2000, 3008, 3)\n",
      "(300, 400, 3)\n",
      "a0058-NKIM_MG_6121.tif\n",
      "(3888, 2592, 3)\n",
      "(300, 400, 3)\n",
      "a0052-dgw_131.tif\n",
      "(2832, 4256, 3)\n",
      "(300, 400, 3)\n"
     ]
    }
   ],
   "source": [
    "composed_transform = transforms.Compose([transforms.ToTensor()])\n",
    "# train_dataset = Dataset (root_dir = '../data', train = True, transform = composed_transform)\n",
    "train_dataset = Dataset (root_dir = '../dataset', train = True, transform = composed_transform)\n",
    "im, im2, im3 = train_dataset.__getitem__(0)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "print (train_dataset.__len__())\n",
    "# print (train_dataset.__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalFeatureNet (nn.Module):\n",
    "           \n",
    "    def __init__(self):\n",
    "        super (LocalFeatureNet, self).__init__()\n",
    "        \n",
    "        self.relu  = nn.ReLU (inplace = True)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d (in_channels = 3,   out_channels = 8, kernel_size = 3, stride = 2, padding = 1)\n",
    "        self.conv2 = nn.Conv2d (in_channels = 8,  out_channels = 16,  kernel_size = 3, stride = 2, padding = 1)\n",
    "        self.conv3 = nn.Conv2d (in_channels = 16, out_channels = 32,  kernel_size= 3, stride = 2,padding = 1)\n",
    "        self.conv4 = nn.Conv2d (in_channels = 32, out_channels = 64,  kernel_size = 3, stride = 2, padding = 1)\n",
    "        \n",
    "        \n",
    "        self.localconv1 = nn.Conv2d (in_channels = 64, out_channels = 64,  kernel_size = 3, stride = 1, padding = 1)\n",
    "        self.localconv2 = nn.Conv2d (in_channels = 64, out_channels = 64,  kernel_size= 3, stride = 1, padding = 1)\n",
    "        \n",
    "        self.globalconv1 = nn.Conv2d (in_channels = 64, out_channels = 64,  kernel_size = 3, stride = 2,padding = 1)\n",
    "        self.globalconv2 = nn.Conv2d (in_channels = 64, out_channels = 64,  kernel_size = 3, stride = 2, padding = 1)\n",
    "        \n",
    "        self.globalfc1 = nn.Linear (1024, 256)\n",
    "        self.globalfc2 = nn.Linear (256, 128)\n",
    "        self.globalfc3 = nn.Linear (128, 64)\n",
    "        self.linear = nn.Conv2d(in_channels = 64, out_channels = 96,  kernel_size = 1, stride = 1)\n",
    "        \n",
    "        # Pixel Wise Network\n",
    "        self.pixelwise_bias = nn.Parameter (torch.rand(3, 1), requires_grad=True)\n",
    "        self.pixelwise_weight = nn.Parameter (torch.eye(3), requires_grad = True)\n",
    "        self.pixelwise_obias = nn.Parameter (torch.eye(1), requires_grad = True)\n",
    "        \n",
    "        self.relu_slopes = nn.Parameter(torch.rand(3,16), requires_grad = True)\n",
    "        self.relu_shifts = nn.Parameter(torch.rand(16,3), requires_grad = True)\n",
    "        \n",
    "    def custom_relu(self,channel,value):\n",
    "        print(\"value\",value.size())\n",
    "        size = (16,value.size()[0],value.size()[1],value.size()[2])\n",
    "        size_alt = (1L,value.size()[0],value.size()[1],value.size()[2])\n",
    "        print(\"size: \",size,\"size_alt\",size_alt)\n",
    "        value = value.expand(size)\n",
    "        \n",
    "#         print(self.relu_shifts[:,channel])\n",
    "        a = self.relu_shifts[:,channel].clone()\n",
    "        a = a.view(16,1,1,1)\n",
    "#         print(a.data,a)\n",
    "#         value = self.relu(value - a.repeat(size_alt))    \n",
    "# Upper line is not working for some damn reason so I hard coded it for now.\n",
    "#         print (\"here \", a.repeat(1,1,img_h,img_w).size())\n",
    "        value = self.relu(value - a.repeat(size[1],1,img_h,img_w))\n",
    "        print(\"st \", value.size())\n",
    "        print (\"st2 \", self.relu_slopes.size())\n",
    "        value = self.relu_slopes.matmul(value.view(img_h,img_w,16,-1))\n",
    "        print (\"stt \", value.size())\n",
    "        value = value.view(-1,3,img_h,img_w)\n",
    "        value = value[:,1,:,:]+value[:,2,:,:]+value[:,0,:,:]\n",
    "        print (\"stt2 \", value.size())\n",
    "        return value\n",
    "    \n",
    "    def upsample(self, grid, bilat):\n",
    "        sx = sy = 16\n",
    "        d = 8\n",
    "#         writer.add_scalar('upsample/sx', sx)\n",
    "        writer.add_scalars('upsample/sizes', {\"sx\": sx,\n",
    "                                              \"sy\": sy,\n",
    "                                              \"d\": d })\n",
    "#         g = grid.clone() \n",
    "        \n",
    "        return torch.rand(1,img_h,img_w,3,4)\n",
    "        \n",
    "    def output(self, grid, inp):\n",
    "        out = torch.rand(inp.size()[0],img_h,img_w,3)\n",
    "        out = 0 * out\n",
    "        grid = grid.view(-1,img_h,img_w,12)\n",
    "        \n",
    "        for i in range(0,3):\n",
    "            out[:,:,:,i] = out[:,:,:,i] + grid[:,:,:,3+4*i]\n",
    "            for j in range(0,3):\n",
    "#                 print(grid[:,:,:,j+4*i].size())\n",
    "#                 print(inp[:,j,:,:].size())\n",
    "                a = torch.autograd.Variable(grid[:,:,:,j+4*i], requires_grad=False)\n",
    "#                 a = a.numpy()\n",
    "                b = inp[:,j,:,:]\n",
    "#                 b = b.numpy()\n",
    "                temp = a.data * b.data\n",
    "#                 print(temp.size())\n",
    "                out[:,:,:,i] = out[:,:,:,i] + temp\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def forward (self, h, l):\n",
    "        \n",
    "#         print (x.size())\n",
    "        \n",
    "        x = self.relu (self.conv1 (l))\n",
    "        print (\"conv1\",x.size())\n",
    "        x = self.relu (self.conv2 (x))\n",
    "        print (\"conv2\",x.size())\n",
    "        x = self.relu (self.conv3 (x))\n",
    "        print (\"conv3\",x.size())\n",
    "        x = self.relu (self.conv4 (x))\n",
    "        print (\"conv4\",x.size())\n",
    "#         print (\"lel\")\n",
    "        y = self.localconv1 (x)\n",
    "#         print (y.size())\n",
    "        y = self.localconv2 (y)\n",
    "        \n",
    "#         print (y.size())\n",
    "        \n",
    "        z = self.globalconv1 (x)\n",
    "        z = self.globalconv2 (z)\n",
    "        z = self.globalfc1 (z.view(z.size()[0], -1))\n",
    "        z = self.globalfc2 (z)\n",
    "        z = self.globalfc3 (z)\n",
    "        \n",
    "#         print (z.size())\n",
    "        \n",
    "        \n",
    "        fused = self.relu(z.view(-1,64,1,1)+y)\n",
    "#         print (fused.size())\n",
    "        \n",
    "        lin = self.linear(fused)\n",
    "#         print (lin.size())\n",
    "        \n",
    "        bilat = lin.view(-1, 8, 3, 4, 16, 16)\n",
    "        \n",
    "        \n",
    "#         bilat = self.bilateralGrid (fused)\n",
    "        \n",
    "        print (\"bilat\",bilat.size())\n",
    "        \n",
    "#         return bilat\n",
    "\n",
    "# pixel wise network\n",
    "        for i in range(0,3):\n",
    "            a = self.pixelwise_weight[i,:].view(-1,3)\n",
    "            b = h.unsqueeze(0).view(3,-1)\n",
    "            print(a.size(),b.size())\n",
    "            p = torch.mm(a,b)\n",
    "            print(p.size())\n",
    "            p = p.view(h.size()[0],h.size()[2],h.size()[3]) + self.pixelwise_bias[i]\n",
    "            print(p.size())\n",
    "#             p = torch.bmm(self.pixelwise_weight[i,:], h.unsqueeze(0).view(3,-1)).view(h.size()) + self.pixelwise_bias[i]\n",
    "            print(i)\n",
    "            p += self.custom_relu(i,p)\n",
    "        p += self.pixelwise_obias\n",
    "        p = p.view(p.size()[0],1,p.size()[1],p.size()[2])\n",
    "        print(\"p\",p.size())\n",
    "        grid = p\n",
    "        writer.add_image('gridmap',grid)\n",
    "        bilat_new = self.upsample(grid, bilat)\n",
    "        \n",
    "        return self.output(bilat_new,h)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LocalFeatureNet() \n",
    "\n",
    "# Add code for using CUDA here if it is available\n",
    "use_gpu = False\n",
    "if(torch.cuda.is_available()):\n",
    "    use_gpu = True\n",
    "    model.cuda()\n",
    "\n",
    "# Loss function and optimizers\n",
    "criterion = torch.nn.MSELoss()# Define MSE loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)# Use Adam optimizer, use learning_rate hyper parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Code for training the model\n",
    "    # Make sure to output a matplotlib graph of training losses\n",
    "    loss_arr = []\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (himage, limage, truth) in enumerate(train_loader):  \n",
    "            # Convert torch tensor to Variable\n",
    "            himage = Variable(himage)\n",
    "            limage = Variable(limage)\n",
    "            truth = Variable(truth)\n",
    "            if(use_gpu):\n",
    "                himage=himage.cuda()\n",
    "                limage=limage.cuda()\n",
    "                truth = truth.cuda()\n",
    "            # Forward + Backward + Optimize\n",
    "            optimizer.zero_grad()  # zero the gradient buffer\n",
    "            outputs = model(himage, limage)\n",
    "            writer.add_image('himage', himage)\n",
    "            writer.add_image('limage', limage)\n",
    "            writer.add_image('truth',truth)\n",
    "            writer.add_image('output', outputs.view(-1,3,img_h,img_w))\n",
    "            loss = criterion(outputs, truth)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_arr.append(loss.data[0])\n",
    "            if (i+1) % batch_size == 0:       \n",
    "                print ('Epoch [%d/%d], Step [%d/%d], Loss: %.4f' \n",
    "                       %(epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.data[0]))\n",
    "    \n",
    "    plt.plot( np.array(range(1,len(loss_arr)+1)), np.array(loss_arr))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1 torch.Size([10, 8, 128, 128])\n",
      "conv2 torch.Size([10, 16, 64, 64])\n",
      "conv3 torch.Size([10, 32, 32, 32])\n",
      "conv4 torch.Size([10, 64, 16, 16])\n",
      "bilat torch.Size([10, 8, 3, 4, 16, 16])\n",
      "torch.Size([1, 3]) torch.Size([3, 1200000])\n",
      "torch.Size([1, 1200000])\n",
      "torch.Size([10, 300, 400])\n",
      "0\n",
      "value torch.Size([10, 300, 400])\n",
      "size:  (16, 10L, 300L, 400L) size_alt (1L, 10L, 300L, 400L)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (16) must match the size of tensor b (160) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-a7b5ea91cbdc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'time train()'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/dash/virtualenvs/pytorch2.7/local/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mmagic\u001b[0;34m(self, arg_s)\u001b[0m\n\u001b[1;32m   2158\u001b[0m         \u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg_s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2159\u001b[0m         \u001b[0mmagic_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmagic_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefilter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mESC_MAGIC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2160\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2162\u001b[0m     \u001b[0;31m#-------------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dash/virtualenvs/pytorch2.7/local/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line)\u001b[0m\n\u001b[1;32m   2079\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2080\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2081\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2082\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m/home/dash/virtualenvs/pytorch2.7/local/lib/python2.7/site-packages/IPython/core/magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dash/virtualenvs/pytorch2.7/local/lib/python2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1187\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'eval'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1189\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1190\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-d15d1572c1eb>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;31m# Forward + Backward + Optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# zero the gradient buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'himage'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'limage'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dash/virtualenvs/pytorch2.7/local/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-79151183fd3f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, h, l)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;31m#             p = torch.bmm(self.pixelwise_weight[i,:], h.unsqueeze(0).view(3,-1)).view(h.size()) + self.pixelwise_bias[i]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0mp\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcustom_relu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m         \u001b[0mp\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpixelwise_obias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-79151183fd3f>\u001b[0m in \u001b[0;36mcustom_relu\u001b[0;34m(self, channel, value)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m# Upper line is not working for some damn reason so I hard coded it for now.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;31m#         print (\"here \", a.repeat(1,1,img_h,img_w).size())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimg_h\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimg_w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"st \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"st2 \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu_slopes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (16) must match the size of tensor b (160) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "%time train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# writer.export_scalars_to_json(\"./all_scalars.json\")\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
