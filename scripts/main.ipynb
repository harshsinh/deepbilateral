{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch implementation of Deep Bilteral Learning for Real Time Image Enhancement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, unicode_literals\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os, sys, glob\n",
    "# import matplotlib.pyplot as plt\n",
    "#Torch Imports\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as data\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "size = (256, 256)\n",
    "batch_size = 100\n",
    "learning_rate = 0.01\n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, root_dir, train, transform=None):\n",
    "        #Init Function\n",
    "        super(Dataset, self).__init__()\n",
    "        self.root_dir = root_dir\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.full_res = []\n",
    "        self.low_res = []\n",
    "        \n",
    "        if (train):\n",
    "            dir = self.root_dir + '/train/'\n",
    "        else :\n",
    "            dir = self.root_dir + '/test/'\n",
    "        \n",
    "        for img_path in glob.glob (dir + '*.jpg'):\n",
    "            \n",
    "            himage = Image.open (img_path)\n",
    "            limage = himage.resize (size)\n",
    "            \n",
    "            self.full_res.append (himage)\n",
    "            self.low_res.append (limage)\n",
    "\n",
    "    def __len__(self):\n",
    "        #Length function ?\n",
    "        return len(self.full_res)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #Accessor Function\n",
    "        if self.transform is None:\n",
    "            return (self.full_res[idx],self.low_res[idx])\n",
    "        else:\n",
    "            limg_transformed = self.transform(self.low_res[idx])\n",
    "            himg_transformed =  self.transform(self.full_res[idx])\n",
    "            return (himg_transformed, limg_transformed)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "composed_transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_dataset = Dataset (root_dir = '../data', train = True, transform = composed_transform)\n",
    "im, im2 = train_dataset.__getitem__(0)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalFeatureNet (nn.Module):\n",
    "           \n",
    "    def __init__(self):\n",
    "        super (LocalFeatureNet, self).__init__()\n",
    "        \n",
    "        self.relu  = nn.ReLU (inplace = True)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d (in_channels = 3,   out_channels = 8, kernel_size = 3, stride = 2, padding = 1)\n",
    "        self.conv2 = nn.Conv2d (in_channels = 8,  out_channels = 16,  kernel_size = 3, stride = 2, padding = 1)\n",
    "        self.conv3 = nn.Conv2d (in_channels = 16, out_channels = 32,  kernel_size= 3, stride = 2,padding = 1)\n",
    "        self.conv4 = nn.Conv2d (in_channels = 32, out_channels = 64,  kernel_size = 3, stride = 2, padding = 1)\n",
    "        \n",
    "        \n",
    "        self.localconv1 = nn.Conv2d (in_channels = 64, out_channels = 64,  kernel_size = 3, stride = 1, padding = 1)\n",
    "        self.localconv2 = nn.Conv2d (in_channels = 64, out_channels = 64,  kernel_size= 3, stride = 1, padding = 1)\n",
    "        \n",
    "        self.globalconv1 = nn.Conv2d (in_channels = 64, out_channels = 64,  kernel_size = 3, stride = 2,padding = 1)\n",
    "        self.globalconv2 = nn.Conv2d (in_channels = 64, out_channels = 64,  kernel_size = 3, stride = 2, padding = 1)\n",
    "        \n",
    "        self.globalfc1 = nn.Linear (1024, 256)\n",
    "        self.globalfc2 = nn.Linear (256, 128)\n",
    "        self.globalfc3 = nn.Linear (128, 64)\n",
    "        self.linear = nn.Conv2d(in_channels = 64, out_channels = 96,  kernel_size = 1, stride = 1)\n",
    "        \n",
    "        # Pixel Wise Network\n",
    "        self.pixelwise_bias = nn.Parameter (torch.rand(3, 1), requires_grad=True)\n",
    "        self.pixelwise_weight = nn.Parameter (torch.eye(3), requires_grad = True)\n",
    "        self.pixelwise_obias = nn.Parameter (torch.eye(1), requires_grad = True)\n",
    "        \n",
    "        self.relu_slopes = nn.Parameter(torch.rand(3,16), requires_grad = True)\n",
    "        self.relu_shifts = nn.Parameter(torch.rand(16,3), requires_grad = True)\n",
    "        \n",
    "    def custom_relu(self,channel,value):\n",
    "        print(\"value\",value.size())\n",
    "        size = (16,value.size()[0],value.size()[1],value.size()[2])\n",
    "        size_alt = (1L,value.size()[0],value.size()[1],value.size()[2])\n",
    "        print(\"size: \",size,\"size_alt\",size_alt)\n",
    "        value = value.expand(size)\n",
    "        \n",
    "        print(self.relu_shifts[:,channel])\n",
    "        a = self.relu_shifts[:,channel].clone()\n",
    "        a = a.view(16,1,1,1)\n",
    "        print(a.data,a)\n",
    "        value = self.relu(value - a.repeat(16,1,1080,1920))\n",
    "        value = self.relu_slopes[channel,:] * value\n",
    "        \n",
    "        return value\n",
    "    \n",
    "    def forward (self, h, l):\n",
    "        \n",
    "#         print (x.size())\n",
    "        \n",
    "        x = self.relu (self.conv1 (l))\n",
    "#         print (x.size())\n",
    "        x = self.relu (self.conv2 (x))\n",
    "#         print (x.size())\n",
    "        x = self.relu (self.conv3 (x))\n",
    "#         print (x.size())\n",
    "        x = self.relu (self.conv4 (x))\n",
    "#         print (x.size())\n",
    "#         print (\"lel\")\n",
    "        y = self.localconv1 (x)\n",
    "#         print (y.size())\n",
    "        y = self.localconv2 (y)\n",
    "        \n",
    "        print (y.size())\n",
    "        \n",
    "        z = self.globalconv1 (x)\n",
    "        z = self.globalconv2 (z)\n",
    "        z = self.globalfc1 (z.view(1, -1))\n",
    "        z = self.globalfc2 (z)\n",
    "        z = self.globalfc3 (z)\n",
    "        \n",
    "        print (z.size())\n",
    "        \n",
    "        \n",
    "        fused = self.relu(z.view(-1,64,1,1)+y)\n",
    "        print (fused.size())\n",
    "        \n",
    "        lin = self.linear(fused)\n",
    "        print (lin.size())\n",
    "        \n",
    "        bilat = lin.view(-1, 8, 3, 4, 16, 16)\n",
    "        \n",
    "        \n",
    "#         bilat = self.bilateralGrid (fused)\n",
    "        \n",
    "        print (bilat.size())\n",
    "        \n",
    "#         return bilat\n",
    "\n",
    "# pixel wise network\n",
    "        for i in range(0,3):\n",
    "            a = self.pixelwise_weight[i,:].view(1,3)\n",
    "            b = h.unsqueeze(0).view(3,-1)\n",
    "            print(a.size(),b.size())\n",
    "            p = torch.mm(a,b)\n",
    "            print(p.size())\n",
    "            p = p.view(h.size()[0],h.size()[2],h.size()[3]) + self.pixelwise_bias[i]\n",
    "            print(p.size())\n",
    "#             p = torch.bmm(self.pixelwise_weight[i,:], h.unsqueeze(0).view(3,-1)).view(h.size()) + self.pixelwise_bias[i]\n",
    "            print(i)\n",
    "            p += self.custom_relu(i,p)\n",
    "        p += self.pixelwise_obias\n",
    "        print(p.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LocalFeatureNet() \n",
    "\n",
    "# Add code for using CUDA here if it is available\n",
    "use_gpu = False\n",
    "if(torch.cuda.is_available()):\n",
    "    use_gpu = True\n",
    "    model.cuda()\n",
    "\n",
    "# Loss function and optimizers\n",
    "criterion = nn.CrossEntropyLoss()# Define cross-entropy loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)# Use Adam optimizer, use learning_rate hyper parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Code for training the model\n",
    "    # Make sure to output a matplotlib graph of training losses\n",
    "    loss_arr = []\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (himage, limage) in enumerate(train_loader):  \n",
    "            # Convert torch tensor to Variable\n",
    "            himage = Variable(himage)\n",
    "            limage = Variable(limage)\n",
    "            if(use_gpu):\n",
    "                himage=himage.cuda()\n",
    "                limage=limage.cuda()\n",
    "            # Forward + Backward + Optimize\n",
    "            optimizer.zero_grad()  # zero the gradient buffer\n",
    "            outputs = model(himage, limage)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             loss_arr.append(loss.data[0])\n",
    "#             if (i+1) % batch_size == 0:       \n",
    "#                 print ('Epoch [%d/%d], Step [%d/%d], Loss: %.4f' \n",
    "#                        %(epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.data[0]))\n",
    "    \n",
    "#     plt.plot( np.array(range(1,len(loss_arr)+1)), np.array(loss_arr))\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 16, 16])\n",
      "torch.Size([1, 64])\n",
      "torch.Size([1, 64, 16, 16])\n",
      "torch.Size([1, 96, 16, 16])\n",
      "torch.Size([1, 8, 3, 4, 16, 16])\n",
      "torch.Size([1, 3]) torch.Size([3, 2073600])\n",
      "torch.Size([1, 2073600])\n",
      "torch.Size([1, 1080, 1920])\n",
      "0\n",
      "value torch.Size([1, 1080, 1920])\n",
      "size:  (16, 1L, 1080L, 1920L) size_alt (1L, 1L, 1080L, 1920L)\n",
      "Variable containing:\n",
      " 0.2890\n",
      " 0.9314\n",
      " 0.8544\n",
      " 0.4124\n",
      " 0.0428\n",
      " 0.4390\n",
      " 0.7779\n",
      " 0.7858\n",
      " 0.1499\n",
      " 0.7652\n",
      " 0.1831\n",
      " 0.5835\n",
      " 0.9061\n",
      " 0.6142\n",
      " 0.7291\n",
      " 0.2037\n",
      "[torch.FloatTensor of size 16]\n",
      "\n",
      "\n",
      "(0 ,0 ,.,.) = \n",
      "  0.2890\n",
      "\n",
      "(1 ,0 ,.,.) = \n",
      "  0.9314\n",
      "\n",
      "(2 ,0 ,.,.) = \n",
      "  0.8544\n",
      "\n",
      "(3 ,0 ,.,.) = \n",
      "  0.4124\n",
      "\n",
      "(4 ,0 ,.,.) = \n",
      "  0.0428\n",
      "\n",
      "(5 ,0 ,.,.) = \n",
      "  0.4390\n",
      "\n",
      "(6 ,0 ,.,.) = \n",
      "  0.7779\n",
      "\n",
      "(7 ,0 ,.,.) = \n",
      "  0.7858\n",
      "\n",
      "(8 ,0 ,.,.) = \n",
      "  0.1499\n",
      "\n",
      "(9 ,0 ,.,.) = \n",
      "  0.7652\n",
      "\n",
      "(10,0 ,.,.) = \n",
      "  0.1831\n",
      "\n",
      "(11,0 ,.,.) = \n",
      "  0.5835\n",
      "\n",
      "(12,0 ,.,.) = \n",
      "  0.9061\n",
      "\n",
      "(13,0 ,.,.) = \n",
      "  0.6142\n",
      "\n",
      "(14,0 ,.,.) = \n",
      "  0.7291\n",
      "\n",
      "(15,0 ,.,.) = \n",
      "  0.2037\n",
      "[torch.FloatTensor of size 16x1x1x1]\n",
      " Variable containing:\n",
      "(0 ,0 ,.,.) = \n",
      "  0.2890\n",
      "\n",
      "(1 ,0 ,.,.) = \n",
      "  0.9314\n",
      "\n",
      "(2 ,0 ,.,.) = \n",
      "  0.8544\n",
      "\n",
      "(3 ,0 ,.,.) = \n",
      "  0.4124\n",
      "\n",
      "(4 ,0 ,.,.) = \n",
      "  0.0428\n",
      "\n",
      "(5 ,0 ,.,.) = \n",
      "  0.4390\n",
      "\n",
      "(6 ,0 ,.,.) = \n",
      "  0.7779\n",
      "\n",
      "(7 ,0 ,.,.) = \n",
      "  0.7858\n",
      "\n",
      "(8 ,0 ,.,.) = \n",
      "  0.1499\n",
      "\n",
      "(9 ,0 ,.,.) = \n",
      "  0.7652\n",
      "\n",
      "(10,0 ,.,.) = \n",
      "  0.1831\n",
      "\n",
      "(11,0 ,.,.) = \n",
      "  0.5835\n",
      "\n",
      "(12,0 ,.,.) = \n",
      "  0.9061\n",
      "\n",
      "(13,0 ,.,.) = \n",
      "  0.6142\n",
      "\n",
      "(14,0 ,.,.) = \n",
      "  0.7291\n",
      "\n",
      "(15,0 ,.,.) = \n",
      "  0.2037\n",
      "[torch.FloatTensor of size 16x1x1x1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%time train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
