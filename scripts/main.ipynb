{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch implementation of Deep Bilteral Learning for Real Time Image Enhancement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, unicode_literals\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os, sys, glob\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "#Torch Imports\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as data\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1920, 16])\n",
      "torch.Size([16, 1080])\n",
      "torch.Size([16, 1080])\n"
     ]
    }
   ],
   "source": [
    "size = (256, 256)\n",
    "batch_size = 100\n",
    "learning_rate = 0.01\n",
    "num_epochs = 5\n",
    "\n",
    "# size of image, right now we are fixing this\n",
    "x_size = 1920\n",
    "y_size = 1080\n",
    "\n",
    "# Constants for slicing layer\n",
    "sx = 256.0/x_size\n",
    "sy = 256.0/y_size\n",
    "d  = 8\n",
    "\n",
    "# tensorboard for pytorch stuff\n",
    "writer = SummaryWriter()\n",
    "sample_rate = 44100\n",
    "freqs = [262, 294, 330, 349, 392, 440, 440, 440, 440, 440, 440]\n",
    "\n",
    "# calculating Fx and keeping as constant from precomputation\n",
    "x = range(x_size)\n",
    "x = np.asarray(x, dtype=np.float64)\n",
    "one = np.ones(x_size)\n",
    "x = np.stack((x, one), axis=1)\n",
    "x[:, 0] = x[:, 0]*sx\n",
    "i  = range(16)\n",
    "i  = np.asarray(i, dtype=np.float64)*-1\n",
    "one= np.ones(16)\n",
    "i = np.stack((one, i), axis=1).swapaxes(0, 1)\n",
    "fx = torch.from_numpy(x)\n",
    "i  = torch.from_numpy(i)\n",
    "fxi= torch.matmul(fx, i)\n",
    "fxi= torch.abs(fxi)\n",
    "print(fxi.shape)\n",
    "\n",
    "# calculating Fy and keeping as constant from precomputation\n",
    "y = range(y_size)\n",
    "y = np.asarray(y, dtype=np.float64)\n",
    "one = np.ones(y_size)\n",
    "y = np.stack((y, one), axis=1)\n",
    "y[:, 0] = y[:, 0]*sy\n",
    "y = y.swapaxes(0, 1)\n",
    "i  = range(16)\n",
    "i  = np.asarray(i, dtype=np.float64)*-1\n",
    "one= np.ones(16)\n",
    "i = np.stack((one, i), axis=1)\n",
    "fy = torch.from_numpy(y)\n",
    "i  = torch.from_numpy(i)\n",
    "fyi= torch.matmul(i, fy)\n",
    "fyi= torch.abs(fyi)\n",
    "print(fyi.shape)\n",
    "print(fyi.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, root_dir, train, transform=None):\n",
    "        #Init Function\n",
    "        super(Dataset, self).__init__()\n",
    "        self.root_dir = root_dir\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.full_res = []\n",
    "        self.low_res = []\n",
    "        self.truth = []\n",
    "        \n",
    "        if (train):\n",
    "            dir = self.root_dir + '/train/'\n",
    "        else :\n",
    "            dir = self.root_dir + '/test/'\n",
    "        \n",
    "#         for img_path in glob.glob (dir + '*.jpg'):\n",
    "        for img_path in glob.glob (dir +'input/' '*.tif'):\n",
    "            img_name = img_path.split('/')[-1]\n",
    "            print (img_name)\n",
    "#             himage = Image.open (img_path)\n",
    "#             himage = himage.resize (img_w,img_h)           \n",
    "#             limage = himage.resize (size)           \n",
    "#             output = Image.open(dir+'output/'+img_name)\n",
    "#             output = output.resize(img_w,img_h)\n",
    "\n",
    "            himage = cv2.imread (img_path)\n",
    "            print(himage.shape)\n",
    "            himage = cv2.resize (himage,(x_size, y_size))\n",
    "            print(himage.shape)\n",
    "            limage = cv2.resize (himage,size)\n",
    "            output = cv2.imread (dir+'output/'+img_name)\n",
    "            output = cv2.resize (output,(x_size, y_size))\n",
    "\n",
    "            \n",
    "            self.full_res.append (himage)\n",
    "            self.low_res.append (limage)\n",
    "            self.truth.append (output)\n",
    "\n",
    "    def __len__(self):\n",
    "        #Length function ?\n",
    "        return len(self.full_res)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #Accessor Function\n",
    "        if self.transform is None:\n",
    "            return (self.full_res[idx],self.low_res[idx])\n",
    "        else:\n",
    "            limg_transformed = self.transform(self.low_res[idx])\n",
    "            himg_transformed =  self.transform(self.full_res[idx])\n",
    "            truth_transformed = self.transform(self.truth[idx])\n",
    "            return (himg_transformed, limg_transformed, truth_transformed)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a0001-jmac_DSC1459.tif\n",
      "(2000, 3008, 3)\n",
      "(1080, 1920, 3)\n",
      "a0004-jmac_MG_1384.tif\n",
      "(2912, 4368, 3)\n",
      "(1080, 1920, 3)\n"
     ]
    }
   ],
   "source": [
    "composed_transform = transforms.Compose([transforms.ToTensor()])\n",
    "# train_dataset = Dataset (root_dir = '../data', train = True, transform = composed_transform)\n",
    "train_dataset = Dataset (root_dir = '../dataset', train = True, transform = composed_transform)\n",
    "# test_dataset = Dataset (root_dir = '../dataset', train = False, transform = composed_transform)\n",
    "# im, im2, im3 = train_dataset.__getitem__(0)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LocalFeatureNet (nn.Module):\n",
    "\n",
    "    def slicing_relu(self, F):\n",
    "        #Function similar to custom relu does the tau calculation\n",
    "        dimension = F.size()\n",
    "        F_temp = F.cuda()\n",
    "        F_temp = torch.ones(dimension).cuda().sub_(torch.abs(F_temp).float())\n",
    "        F_temp = F_temp.cpu().numpy()\n",
    "        F_temp[F_temp < 0] = 0\n",
    "        F_temp = torch.from_numpy(F_temp)\n",
    "        return F_temp\n",
    "    \n",
    "    def __init__(self):\n",
    "        super (LocalFeatureNet, self).__init__()\n",
    "        \n",
    "        self.relu  = nn.ReLU (inplace = True)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d (in_channels = 3,   out_channels = 8, kernel_size = 3, stride = 2, padding = 1)\n",
    "        self.conv2 = nn.Conv2d (in_channels = 8,  out_channels = 16,  kernel_size = 3, stride = 2, padding = 1)\n",
    "        self.conv3 = nn.Conv2d (in_channels = 16, out_channels = 32,  kernel_size= 3, stride = 2,padding = 1)\n",
    "        self.conv4 = nn.Conv2d (in_channels = 32, out_channels = 64,  kernel_size = 3, stride = 2, padding = 1)\n",
    "        \n",
    "        \n",
    "        self.localconv1 = nn.Conv2d (in_channels = 64, out_channels = 64,  kernel_size = 3, stride = 1, padding = 1)\n",
    "        self.localconv2 = nn.Conv2d (in_channels = 64, out_channels = 64,  kernel_size= 3, stride = 1, padding = 1)\n",
    "        \n",
    "        self.globalconv1 = nn.Conv2d (in_channels = 64, out_channels = 64,  kernel_size = 3, stride = 2,padding = 1)\n",
    "        self.globalconv2 = nn.Conv2d (in_channels = 64, out_channels = 64,  kernel_size = 3, stride = 2, padding = 1)\n",
    "        \n",
    "        self.globalfc1 = nn.Linear (1024, 256)\n",
    "        self.globalfc2 = nn.Linear (256, 128)\n",
    "        self.globalfc3 = nn.Linear (128, 64)\n",
    "        self.linear = nn.Conv2d(in_channels = 64, out_channels = 96,  kernel_size = 1, stride = 1)\n",
    "        \n",
    "        # Pixel Wise Network\n",
    "        self.pixelwise_bias = nn.Parameter (torch.rand(3, 1), requires_grad=True)\n",
    "        self.pixelwise_weight = nn.Parameter (torch.eye(3), requires_grad = True)\n",
    "        self.pixelwise_obias = nn.Parameter (torch.eye(1), requires_grad = True)\n",
    "        \n",
    "        self.relu_slopes = nn.Parameter(torch.rand(3,16), requires_grad = True)\n",
    "        self.relu_shifts = nn.Parameter(torch.rand(16,3), requires_grad = True)\n",
    "        \n",
    "        # Thing we do for upsampling\n",
    "        self.Fx = fxi\n",
    "        self.Fy = fyi\n",
    "        self.Fx = self.slicing_relu(self.Fx).cuda()\n",
    "        self.Fy = self.slicing_relu(self.Fy).cuda()\n",
    "        \n",
    "        \n",
    "    def custom_relu(self,channel,value):\n",
    "        print(\"value\",value.size())\n",
    "        size = (16,value.size()[0],value.size()[1],value.size()[2])\n",
    "        size_alt = (1L,value.size()[0],value.size()[1],value.size()[2])\n",
    "        print(\"size: \",size,\"size_alt\",size_alt)\n",
    "        value = value.expand(size)\n",
    "        \n",
    "#         print(self.relu_shifts[:,channel])\n",
    "        a = self.relu_shifts[:,channel].clone()\n",
    "        a = a.view(16,1,1,1)\n",
    "#         print(a.data,a)\n",
    "#         value = self.relu(value - a.repeat(size_alt))    \n",
    "# Upper line is not working for some damn reason so I hard coded it for now.\n",
    "#         print (\"here \", a.repeat(1,1,img_h,img_w).size())\n",
    "        value = self.relu(value - a.repeat(size[1],1, x_size, y_size))\n",
    "        print(\"st \", value.size())\n",
    "        print (\"st2 \", self.relu_slopes.size())\n",
    "        value = self.relu_slopes.matmul(value.view(x_size, y_size,16,-1))\n",
    "        print (\"stt \", value.size())\n",
    "        value = value.view(-1,3, x_size, y_size)\n",
    "        value = value[:,1,:,:]+value[:,2,:,:]+value[:,0,:,:]\n",
    "        print (\"stt2 \", value.size())\n",
    "        return value\n",
    "    \n",
    "    def upsample(self, grid, bilat):\n",
    "        \n",
    "        #### Making G\n",
    "        g = grid.cuda()\n",
    "        a = bilat.cuda()\n",
    "        print(type(bilat))\n",
    "        G = g.data\n",
    "        print(type(G))\n",
    "        G = d*G\n",
    "        one = torch.ones(G.shape).cuda()\n",
    "        print(type(one))\n",
    "        G = torch.stack([G, one], 3)\n",
    "        print(G.shape)\n",
    "        one = torch.ones(8)\n",
    "        K = torch.range(0, 7).cuda()* -1\n",
    "        one= torch.ones(8).cuda()\n",
    "        K = torch.stack((K, one), 1).permute(1, 0)\n",
    "        z = K[0]\n",
    "        K[0] = K[1]\n",
    "        K[1] = z\n",
    "#         G = torch.from_numpy(G)\n",
    "        F_gk = torch.matmul(G, K)\n",
    "        F_g = self.slicing_relu(F_gk)\n",
    "        print(\"**********************\")\n",
    "        print(F_gk.size())\n",
    "        print(F_g.size())\n",
    "        print(\"**********************\")\n",
    "        #########################################\n",
    "        \n",
    "        print(a.data.size())\n",
    "        A_temp = a.data.permute(0, 2, 3, 1, 4, 5)\n",
    "        F_prod = torch.matmul(self.Fx, A_temp)\n",
    "        F_prod = torch.matmul(F_prod, self.Fy)\n",
    "        print(\"**********************\")\n",
    "        print(F_prod.size())\n",
    "        print(\"**********************\")\n",
    "        F_prod_sum = F_prod.sum(3)\n",
    "        print(\"$$$$$$$$$$$$$$$\")\n",
    "        print(F_prod_sum.size())\n",
    "        print(\"**********************\")\n",
    "        a.data = F_prod_sum\n",
    "        return F_prod_sum\n",
    "        \n",
    "        \n",
    "        \n",
    "    def output(self, grid, inp):\n",
    "        out = torch.rand(inp.size()[0], y_size, x_size, 3).cuda()\n",
    "        out = 0 * out\n",
    "        grid = grid.view(-1, y_size, x_size, 12)\n",
    "        \n",
    "        for i in range(0,3):\n",
    "            out[:,:,:,i] = out[:,:,:,i].add_(grid[:,:,:,3+4*i])\n",
    "            for j in range(0,3):\n",
    "#                 print(grid[:,:,:,j+4*i].size())\n",
    "#                 print(inp[:,j,:,:].size())\n",
    "                a = torch.autograd.Variable(grid[:,:,:,j+4*i], requires_grad=False)\n",
    "#                 a = a.numpy()\n",
    "                b = inp[:,j,:,:]\n",
    "#                 b = b.numpy()\n",
    "                temp = a.data * b.data\n",
    "#                 print(temp.size())\n",
    "                out[:,:,:,i] = out[:,:,:,i] + temp\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def forward (self, h, l):\n",
    "        \n",
    "        x = self.relu (self.conv1 (l))\n",
    "        x = self.relu (self.conv2 (x))\n",
    "        x = self.relu (self.conv3 (x))\n",
    "        x = self.relu (self.conv4 (x))\n",
    "        y = self.localconv1 (x)\n",
    "        y = self.localconv2 (y)\n",
    "        z = self.globalconv1 (x)\n",
    "        z = self.globalconv2 (z)\n",
    "        z = self.globalfc1 (z.view(z.size()[0], -1))\n",
    "        z = self.globalfc2 (z)\n",
    "        z = self.globalfc3 (z)\n",
    "        fused = self.relu(z.view(-1,64,1,1)+y)\n",
    "        lin = self.linear(fused)\n",
    "        bilat = lin.view(-1, 8, 3, 4, 16, 16)\n",
    "#         print (bilat.size())\n",
    "\n",
    "# pixel wise network\n",
    "        for i in range(0,3):\n",
    "            a = self.pixelwise_weight[i,:].view(-1,3)\n",
    "            b = h.unsqueeze(0).view(3,-1)\n",
    "#             print(a.size(),b.size())\n",
    "            p = torch.mm(a,b)\n",
    "#             print(p.size())\n",
    "            p = p.view(h.size()[0],h.size()[2],h.size()[3]) + self.pixelwise_bias[i]\n",
    "#             print(p.size())\n",
    "#             p = torch.bmm(self.pixelwise_weight[i,:], h.unsqueeze(0).view(3,-1)).view(h.size()) + self.pixelwise_bias[i]\n",
    "#             print(i)\n",
    "            p += self.custom_relu(i,p)\n",
    "        p += self.pixelwise_obias\n",
    "#         print(p.size())\n",
    "        grid = p\n",
    "#         print(\"+++++++++++++++++++\")\n",
    "#         print(bilat.size())\n",
    "#         print(\"+++++++++++++++++++\")\n",
    "        print(grid.size())\n",
    "        bilat_new = self.upsample(grid, bilat)\n",
    "        #p is modified for tensorboard\n",
    "        p = p.view(p.size()[0],1,p.size()[1],p.size()[2])\n",
    "        writer.add_image('gridmap', p)\n",
    "        return self.output(bilat_new,h)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LocalFeatureNet() \n",
    "\n",
    "# Add code for using CUDA here if it is available\n",
    "use_gpu = False\n",
    "if(torch.cuda.is_available()):\n",
    "    use_gpu = True\n",
    "    model.cuda()\n",
    "\n",
    "# Loss function and optimizers\n",
    "criterion = torch.nn.MSELoss()# Define MSE loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)# Use Adam optimizer, use learning_rate hyper parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Code for training the model\n",
    "    # Make sure to output a matplotlib graph of training losses\n",
    "    loss_arr = []\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (himage, limage, truth) in enumerate(train_loader):  \n",
    "            \n",
    "            # Convert torch tensor to Variable\n",
    "            himage = Variable(himage)\n",
    "            limage = Variable(limage)\n",
    "            truth = Variable(truth)\n",
    "            if(use_gpu):\n",
    "                himage=himage.cuda()\n",
    "                limage=limage.cuda()\n",
    "                truth = truth.cuda()\n",
    "            \n",
    "            # Forward + Backward + Optimize\n",
    "            optimizer.zero_grad()  # zero the gradient buffer\n",
    "            outputs = model(himage, limage)\n",
    "            writer.add_image('himage', himage)\n",
    "            writer.add_image('limage', limage)\n",
    "            writer.add_image('truth',truth)\n",
    "            writer.add_image('output', outputs.view(-1,3, y_size, x_size))\n",
    "            loss = criterion(outputs, truth)\n",
    "            \n",
    "            model.pixelwise_obias.backward()\n",
    "            \n",
    "            model.pixelwise_bias[0,0].backward()\n",
    "            model.pixelwise_bias[1,0].backward()\n",
    "            model.pixelwise_bias[2,0].backward()\n",
    "            \n",
    "            model.pixelwise_weight[0,0].backward()\n",
    "            model.pixelwise_weight[0,1].backward()\n",
    "            model.pixelwise_weight[0,2].backward()\n",
    "            model.pixelwise_weight[1,0].backward()\n",
    "            model.pixelwise_weight[1,1].backward()\n",
    "            model.pixelwise_weight[1,2].backward()\n",
    "            model.pixelwise_weight[2,0].backward()\n",
    "            model.pixelwise_weight[2,1].backward()\n",
    "            model.pixelwise_weight[2,2].backward()\n",
    "            \n",
    "            for i in range(3):\n",
    "                for j in range(16):\n",
    "                    model.relu_slopes[i,j].backward()\n",
    "                    model.relu_shifts[j,i].backward()\n",
    "                    \n",
    "#             loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_arr.append(loss.data[0])\n",
    "            writer.add_scalar('loss',loss.data[0])\n",
    "            if (i+1) % batch_size == 0:       \n",
    "                print ('Epoch [%d/%d], Step [%d/%d], Loss: %.4f' \n",
    "                       %(epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.data[0]))\n",
    "    \n",
    "    plt.plot( np.array(range(1,len(loss_arr)+1)), np.array(loss_arr))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value torch.Size([2, 1080, 1920])\n",
      "size:  (16, 2L, 1080L, 1920L) size_alt (1L, 2L, 1080L, 1920L)\n",
      "st  torch.Size([16, 2, 1080, 1920])\n",
      "st2  torch.Size([3, 16])\n",
      "stt  torch.Size([1920, 1080, 3, 2])\n",
      "stt2  torch.Size([2, 1920, 1080])\n",
      "value torch.Size([2, 1080, 1920])\n",
      "size:  (16, 2L, 1080L, 1920L) size_alt (1L, 2L, 1080L, 1920L)\n",
      "st  torch.Size([16, 2, 1080, 1920])\n",
      "st2  torch.Size([3, 16])\n",
      "stt  torch.Size([1920, 1080, 3, 2])\n",
      "stt2  torch.Size([2, 1920, 1080])\n",
      "value torch.Size([2, 1080, 1920])\n",
      "size:  (16, 2L, 1080L, 1920L) size_alt (1L, 2L, 1080L, 1920L)\n",
      "st  torch.Size([16, 2, 1080, 1920])\n",
      "st2  torch.Size([3, 16])\n",
      "stt  torch.Size([1920, 1080, 3, 2])\n",
      "stt2  torch.Size([2, 1920, 1080])\n",
      "torch.Size([2, 1080, 1920])\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "<class 'torch.cuda.FloatTensor'>\n",
      "<class 'torch.cuda.FloatTensor'>\n",
      "torch.Size([2, 1080, 1920, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eeshangd/vision_env/lib/python2.7/site-packages/ipykernel_launcher.py:88: UserWarning: torch.range is deprecated in favor of torch.arange and will be removed in 0.3. Note that arange generates values in [start; end), not [start; end].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********************\n",
      "torch.Size([2, 1080, 1920, 8])\n",
      "torch.Size([2, 1080, 1920, 8])\n",
      "**********************\n",
      "torch.Size([2, 8, 3, 4, 16, 16])\n",
      "**********************\n",
      "torch.Size([2, 3, 4, 8, 1920, 1080])\n",
      "**********************\n",
      "$$$$$$$$$$$$$$$\n",
      "torch.Size([2, 3, 4, 1920, 1080])\n",
      "**********************\n",
      "value torch.Size([2, 1080, 1920])\n",
      "size:  (16, 2L, 1080L, 1920L) size_alt (1L, 2L, 1080L, 1920L)\n",
      "st  torch.Size([16, 2, 1080, 1920])\n",
      "st2  torch.Size([3, 16])\n",
      "stt  torch.Size([1920, 1080, 3, 2])\n",
      "stt2  torch.Size([2, 1920, 1080])\n",
      "value torch.Size([2, 1080, 1920])\n",
      "size:  (16, 2L, 1080L, 1920L) size_alt (1L, 2L, 1080L, 1920L)\n",
      "st  torch.Size([16, 2, 1080, 1920])\n",
      "st2  torch.Size([3, 16])\n",
      "stt  torch.Size([1920, 1080, 3, 2])\n",
      "stt2  torch.Size([2, 1920, 1080])\n",
      "value torch.Size([2, 1080, 1920])\n",
      "size:  (16, 2L, 1080L, 1920L) size_alt (1L, 2L, 1080L, 1920L)\n",
      "st  torch.Size([16, 2, 1080, 1920])\n",
      "st2  torch.Size([3, 16])\n",
      "stt  torch.Size([1920, 1080, 3, 2])\n",
      "stt2  torch.Size([2, 1920, 1080])\n",
      "torch.Size([2, 1080, 1920])\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "<class 'torch.cuda.FloatTensor'>\n",
      "<class 'torch.cuda.FloatTensor'>\n",
      "torch.Size([2, 1080, 1920, 2])\n",
      "**********************\n",
      "torch.Size([2, 1080, 1920, 8])\n",
      "torch.Size([2, 1080, 1920, 8])\n",
      "**********************\n",
      "torch.Size([2, 8, 3, 4, 16, 16])\n",
      "**********************\n",
      "torch.Size([2, 3, 4, 8, 1920, 1080])\n",
      "**********************\n",
      "$$$$$$$$$$$$$$$\n",
      "torch.Size([2, 3, 4, 1920, 1080])\n",
      "**********************\n",
      "value torch.Size([2, 1080, 1920])\n",
      "size:  (16, 2L, 1080L, 1920L) size_alt (1L, 2L, 1080L, 1920L)\n",
      "st  torch.Size([16, 2, 1080, 1920])\n",
      "st2  torch.Size([3, 16])\n",
      "stt  torch.Size([1920, 1080, 3, 2])\n",
      "stt2  torch.Size([2, 1920, 1080])\n",
      "value torch.Size([2, 1080, 1920])\n",
      "size:  (16, 2L, 1080L, 1920L) size_alt (1L, 2L, 1080L, 1920L)\n",
      "st  torch.Size([16, 2, 1080, 1920])\n",
      "st2  torch.Size([3, 16])\n",
      "stt  torch.Size([1920, 1080, 3, 2])\n",
      "stt2  torch.Size([2, 1920, 1080])\n",
      "value torch.Size([2, 1080, 1920])\n",
      "size:  (16, 2L, 1080L, 1920L) size_alt (1L, 2L, 1080L, 1920L)\n",
      "st  torch.Size([16, 2, 1080, 1920])\n",
      "st2  torch.Size([3, 16])\n",
      "stt  torch.Size([1920, 1080, 3, 2])\n",
      "stt2  torch.Size([2, 1920, 1080])\n",
      "torch.Size([2, 1080, 1920])\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "<class 'torch.cuda.FloatTensor'>\n",
      "<class 'torch.cuda.FloatTensor'>\n",
      "torch.Size([2, 1080, 1920, 2])\n",
      "**********************\n",
      "torch.Size([2, 1080, 1920, 8])\n",
      "torch.Size([2, 1080, 1920, 8])\n",
      "**********************\n",
      "torch.Size([2, 8, 3, 4, 16, 16])\n",
      "**********************\n",
      "torch.Size([2, 3, 4, 8, 1920, 1080])\n",
      "**********************\n",
      "$$$$$$$$$$$$$$$\n",
      "torch.Size([2, 3, 4, 1920, 1080])\n",
      "**********************\n",
      "value torch.Size([2, 1080, 1920])\n",
      "size:  (16, 2L, 1080L, 1920L) size_alt (1L, 2L, 1080L, 1920L)\n",
      "st  torch.Size([16, 2, 1080, 1920])\n",
      "st2  torch.Size([3, 16])\n",
      "stt  torch.Size([1920, 1080, 3, 2])\n",
      "stt2  torch.Size([2, 1920, 1080])\n",
      "value torch.Size([2, 1080, 1920])\n",
      "size:  (16, 2L, 1080L, 1920L) size_alt (1L, 2L, 1080L, 1920L)\n",
      "st  torch.Size([16, 2, 1080, 1920])\n",
      "st2  torch.Size([3, 16])\n",
      "stt  torch.Size([1920, 1080, 3, 2])\n",
      "stt2  torch.Size([2, 1920, 1080])\n",
      "value torch.Size([2, 1080, 1920])\n",
      "size:  (16, 2L, 1080L, 1920L) size_alt (1L, 2L, 1080L, 1920L)\n",
      "st  torch.Size([16, 2, 1080, 1920])\n",
      "st2  torch.Size([3, 16])\n",
      "stt  torch.Size([1920, 1080, 3, 2])\n",
      "stt2  torch.Size([2, 1920, 1080])\n",
      "torch.Size([2, 1080, 1920])\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "<class 'torch.cuda.FloatTensor'>\n",
      "<class 'torch.cuda.FloatTensor'>\n",
      "torch.Size([2, 1080, 1920, 2])\n",
      "**********************\n",
      "torch.Size([2, 1080, 1920, 8])\n",
      "torch.Size([2, 1080, 1920, 8])\n",
      "**********************\n",
      "torch.Size([2, 8, 3, 4, 16, 16])\n",
      "**********************\n",
      "torch.Size([2, 3, 4, 8, 1920, 1080])\n",
      "**********************\n",
      "$$$$$$$$$$$$$$$\n",
      "torch.Size([2, 3, 4, 1920, 1080])\n",
      "**********************\n",
      "value torch.Size([2, 1080, 1920])\n",
      "size:  (16, 2L, 1080L, 1920L) size_alt (1L, 2L, 1080L, 1920L)\n",
      "st  torch.Size([16, 2, 1080, 1920])\n",
      "st2  torch.Size([3, 16])\n",
      "stt  torch.Size([1920, 1080, 3, 2])\n",
      "stt2  torch.Size([2, 1920, 1080])\n",
      "value torch.Size([2, 1080, 1920])\n",
      "size:  (16, 2L, 1080L, 1920L) size_alt (1L, 2L, 1080L, 1920L)\n",
      "st  torch.Size([16, 2, 1080, 1920])\n",
      "st2  torch.Size([3, 16])\n",
      "stt  torch.Size([1920, 1080, 3, 2])\n",
      "stt2  torch.Size([2, 1920, 1080])\n",
      "value torch.Size([2, 1080, 1920])\n",
      "size:  (16, 2L, 1080L, 1920L) size_alt (1L, 2L, 1080L, 1920L)\n",
      "st  torch.Size([16, 2, 1080, 1920])\n",
      "st2  torch.Size([3, 16])\n",
      "stt  torch.Size([1920, 1080, 3, 2])\n",
      "stt2  torch.Size([2, 1920, 1080])\n",
      "torch.Size([2, 1080, 1920])\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "<class 'torch.cuda.FloatTensor'>\n",
      "<class 'torch.cuda.FloatTensor'>\n",
      "torch.Size([2, 1080, 1920, 2])\n",
      "**********************\n",
      "torch.Size([2, 1080, 1920, 8])\n",
      "torch.Size([2, 1080, 1920, 8])\n",
      "**********************\n",
      "torch.Size([2, 8, 3, 4, 16, 16])\n",
      "**********************\n",
      "torch.Size([2, 3, 4, 8, 1920, 1080])\n",
      "**********************\n",
      "$$$$$$$$$$$$$$$\n",
      "torch.Size([2, 3, 4, 1920, 1080])\n",
      "**********************\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEDCAYAAAAlRP8qAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f28846b57d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 39.7 s, sys: 3.2 s, total: 42.9 s\n",
      "Wall time: 28.5 s\n"
     ]
    }
   ],
   "source": [
    "%time train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), 'model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load('model.pkl'))\n",
    "def train():\n",
    "    # Code for training the model\n",
    "    # Make sure to output a matplotlib graph of training losses\n",
    "    loss_arr = []\n",
    "    for i, (himage, limage, truth) in enumerate(test_loader):  \n",
    "\n",
    "        # Convert torch tensor to Variable\n",
    "        himage = Variable(himage)\n",
    "        limage = Variable(limage)\n",
    "        truth = Variable(truth)\n",
    "        if(use_gpu):\n",
    "            himage=himage.cuda()\n",
    "            limage=limage.cuda()\n",
    "            truth = truth.cuda()\n",
    "\n",
    "\n",
    "        outputs = model(himage, limage)\n",
    "        writer.add_image('test/himage', himage)\n",
    "        writer.add_image('test/limage', limage)\n",
    "        writer.add_image('test/truth',truth)\n",
    "        writer.add_image('test/output', outputs.view(-1,3, y_size, x_size))\n",
    "        loss = criterion(outputs, truth)\n",
    "\n",
    "        loss_arr.append(loss.data[0])\n",
    "        writer.add_scalar('test/loss',loss.data[0])\n",
    "    \n",
    "    plt.plot( np.array(range(1,len(loss_arr)+1)), np.array(loss_arr))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# writer.export_scalars_to_json(\"./all_scalars.json\")\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vision_env",
   "language": "python",
   "name": "vision_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
